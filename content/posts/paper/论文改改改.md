---
title: "论文改改改"
date: "2021-01-27T00:00:00+08:00"
tags: paper
categories: paper
draft: true
---

## 第二版论文思路

1. 先用GJSPCA缩小异常微服务的范围得到候选异常微服务集合A（类似MicroRCA中的聚类）
2. 对于A中的每个微服务m，GJSPCA会得到一个score1
3. 拓展A中任意两个微服务之间的路径组成异常子图：集合A+（类似MicroHECL的剪枝、MicroRCA中的Subgraph）
4. 然后借用MicroHECL中的异常传播方向给集合A+中的每个微服务的打分进行累加
5. 对于cpu、throughput、error count等metric，计算Corr，更新score2为score2*max_Corr
6. final_score = ∂*score1 + (1-∂)score2，作为PPR的输入
7. PPR得到最终的微服务异常排序。并且可以参考Corr进行异常排查。

问题：

- GJSPCA和普通的PCA的区别在哪里？JSPCA变稀疏，加入G即加入了拉普拉斯矩阵图信息
- score1表示的是什么？对于新基距离最大的那个source
- MicroHECL对异常传播方向是怎么研究的？通过异常传播链的分析，可以得到一系列的候选异常根因服务。
- GJSPCA和聚类还有相似度计算相比优势在哪里？

## 其他论文可借鉴部分

- IWQoS 2020 MicroCause：architect好像放后面比较合适？
- HECL：arc不是我那样画的，最好把我之前的框框图和流程图结合到一起
- AutoMAP：图一定要用svg的，哪怕分辨率从72调成300，也给人一种低级的感觉。

## 异常情况分类

1. 硬件异常：cpu、mem、network、磁盘等异常
2. 内部异常：服务内部错误调用比较多（参数错误导致bug？），即错误异常比较多
3. 流量异常：throughput出现问题，负载均衡没做好也属于这一类

反映在响应时间上都会出现RT增大或者服务不可用。其中Zipkin可以cover的仅有2。

1硬件异常，可能会导致某些微服务的响应时间异常，如cpu敏感、磁盘敏感的微服务。但是不是微服务出现异常而是硬件出现异常，这部分Zipkin不能cover。3流量异常，在实验的过程中给一个服务增加流量负载，不是这个服务出现异常，而应该是流量出现异常。

## Metric分类

根据AutoMAP这篇论文，Metrics有：Latency、Throughput、CPU、Memory、IO、Power、Avaliability

根据MicroHECL这篇论文，Metics有：EC(Error Count)、RT(Rsponse Time)、QPS(Querys Per Second)

## 测试分类

| |   Throughput   |   CPU  | Memory | IO | 其他网络 |
| ----| :--: | :--: | :--: | :--: | :--: |
|主机 | / | 部分影响 | 都有影响 | fe到checkout有影响<br />ppage到recom有影响 | 直接影响 |
|容器配置 | 直接影响 | 部分影响 | 直接影响 | / | / |

## Stree方法

增加主机内存负载：

```shell
$ stress --vm 6 --vm-bytes 1G --vm-hang 50 --timeout 60s
```

增加主机CPU负载：

```shell
$ stress -c 4 -t 60
```

查看容器CPU情况：

```mathematica
sum (rate (container_cpu_usage_seconds_total{}[1m])) / sum (machine_cpu_cores{}) * 100
```

然后可以发现如果增加主机CPU负载，容器的CPU也会受到影响，但是反应在响应时间上没有什么变化。



然后我把currencyservice的cpu给限制了：（限制hipster的CPU需要到`release/kubernetes-menifest.yaml`文件里修改）

```yaml
resources:
          requests:
            cpu: 100m
            memory: 64Mi
          limits:
            cpu: 100m
            memory: 64Mi
```

然后并没有什么影响……

我遇到一个聚类不太好解决的地方，就是如果某一个微服务可能因为不清楚的原因响应时间出现波动，聚类很快就能看出来，但是没有任何异常发生，这就像一个误报。这种误报呈大起大落式的锥子。

**latency变化最大的那个微服务不一定就是出现问题的微服务**，这是我今天的实验里看到的最直观的反映。我把currency的delay设为0.3，然后fe、load、cart、checkout等微服务的响应时间远远超过currency。另外checkout这个服务总是不知道因为什么响应时间暴增。

istio-telemetry这个服务一挂，整个istio的监控就失效了。

还有P多少比较合适，不知道是不是我1201机子的问题，我的P90波动程度很大，就有点…



### 参考

https://blog.csdn.net/qq_31555951/article/details/108984036 （k8s监控PromQL）

https://www.cnblogs.com/gavin11/p/12612082.html （大部分的k8s监控指标都列出来了）