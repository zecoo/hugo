<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>paper on This is a title</title>
    <link>https://zecoo.github.io/hugo/categories/paper/</link>
    <description>Recent content in paper on This is a title</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 22 May 2020 00:00:00 +0800</lastBuildDate>
    
	<atom:link href="https://zecoo.github.io/hugo/categories/paper/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Scenario Driven MS Decomposition</title>
      <link>https://zecoo.github.io/hugo/posts/paper/scenario-driven-ms-decomposition/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://zecoo.github.io/hugo/posts/paper/scenario-driven-ms-decomposition/</guid>
      <description>场景驱动、自底向上的单体系统微服务拆分方法
评价 核心思想：把数据表之间紧密联系的聚在一起，作为拆分微服务的准则。
实际上是关于数据表的划分，微服务的其他方面比如class、method的划分没有提到。
AIBR 结合Scenario、Trace和Sql进行微服务拆分。
单体系统的微服务化拆分也可以借鉴传统软件模块化的思路和方法,已有不少这方面的研究
图1是整个流程图
 标记权重：对用例标记权重（对于一些使用频率较高的用例增加权重）用例权重增加，用例包含的trace中边的权重都增加 监控工具：Kieker，收集trace的工具 日志类型：被调用方法的签名、调用链 ID、调用时间、调用顺序 轨迹图：见下图图3 共享群组+关联度矩阵（后面会提） 数据表权重图（关联矩阵添加权重的结果，用于聚类） 数据图表聚类（GN社区聚类算法） 计算拆分开销（later） 拆分方案生产  拆分方法 图3也很关键，一个scenario数据访问的trace。
同样还有图4，数据库的相关度graph（later）
数据表关联度 表关联度高表示表A和表B在一个场景要同时使用的情况比较多。比如说注册要用到user表和profile表。登录也要用到这两个表。就说明user表和profile表的关联度较高。
 sql的权重 两个数据表之间的关联度  表A和表B的关联度 = Cscenario + Ctrace + Csql
Cscenario = 同时操作表A+表B的场景之和 / 操作表A或表B中任意一张表的场景之和，trace、sql同理
共享群组 共享度高表示一张表被多个场景使用，更倾向于单独拆分成一个微服务。比如account表在注册场景、下单场景都用到了，说明account表的共享度较高。
共享度 = Sscenario + Strace + Ssql
Sscenario = 操作表t的场景数量 / 总场景数量，trace、sql同理
共享表的数量 = 共享表占比 * 所有表的数量
那么共享表占比是如何计算出来的呢？（没有提，可能是共享度达到某个值以上的就算是共享表吧）
表依赖度 和关联度、共享度类似，表示表A对表B的依赖程度。那和关联度有什么区别呢…依赖度高，表A和表B更有可能有一种主从关系。见图6
Dscenario = 同时操作表A和表B的场景 / 操作表A的场景之和（Tb对Ta的依赖）
共享群组条件 Ta和Tb的相互依赖程度（Ta -&amp;gt; Tb + Tb -&amp;gt; Ta）大于某一个值</description>
    </item>
    
    <item>
      <title>Delta debug MS</title>
      <link>https://zecoo.github.io/hugo/posts/paper/delta-debug-ms/</link>
      <pubDate>Wed, 20 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://zecoo.github.io/hugo/posts/paper/delta-debug-ms/</guid>
      <description> a triangular tract of sediment deposited at the mouth of a river, typically where it diverges into several outlets.  the fourth letter of the Greek alphabet ( Δ , δ ), transliterated as ‘d.’.  variation of a variable or function.  一组影响系统执行dimensions，高效找出导致错误的deltas。这里delta包括（deployment、enviromental、configurations）  四个dimensions：node、instance、configuration、sequence  node：分布式的服务器，未知性较多  instance：微服务的instance一般都是有状态的，如果没有处理好，相同的instance如果不是同样的状态就会出错。  configuration：docker的配置，k8s的配置，是挺恶心的，万一内存不够，就爆了  sequence：异步的invocation导致错误  微服务settings作为circumstances，就可以用delta bebugging了，db是一种简化和隔离错误案例的方法  Istio是微服务service mesh的？Istio可以部署在k8s里的  delta debugging的作用是找到min的deltas导致错误  controller：delta debugging的位置，是文章核心嗷  scheduler：根据容器状态选择test case去执行。queue，有可用资源就去跑测试  executor：根据所给的circumstances在docker上跑测试用例，返回测试结果  circumstance：4个dimension的组合  delta：两个cir之间的差异  delta db目的：通过最简单cir抽取导致错误的最小delta set  min setting：一个node，一个instance，默认conf（full memory），正确的seq  general setting：一个可能引起错误的cir  表示方法：0表示min setting，1表示general setting  seq的表示方法稍微复杂一点，0表示顺序执行，1表示逆序执行  seq能全覆盖，node、instance numbers可以放低一点要求  If the given failing test case still fails with the simplest circumstance, the failure can be thought to be caused by internal faults of related microservices  我们的目标是找到应用在min cir上导致ftc产生ftr的同时ptc产生ptr的deltas  atomic delta是什么意思？？  we partition the set of deltas X into n equal-sized partitions  13vm each 8-core cpu 24gb memory  36-63 deltas -》 1-2deltas  就是能分出到底是哪种问题  但是有个很重要的点就是要开发再确认bug   </description>
    </item>
    
    <item>
      <title>Graph-based RCA in SOA and MS</title>
      <link>https://zecoo.github.io/hugo/posts/paper/graph-based-rca-in-soa-and-ms/</link>
      <pubDate>Wed, 20 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://zecoo.github.io/hugo/posts/paper/graph-based-rca-in-soa-and-ms/</guid>
      <description>Graph-based root cause analysis for service-oriented and microservice architectures
JSS 2020
2 RW RCA方法有以下大致分类
 model-based的RCA方法 Spectrum（也可以称之为Classification）方法。有人用ML来做 图的方法  2017年的Sieve，MS-Rank也提到了这篇论文。
3 Graph-based RCA 举一个Wordpress的例子，如果不做loadbalance，HAProxy的运行缓慢可能是其中某一个服务器负载过大。
Graph Modules Node：
Edge：网络连接的对象，例如一次TCP连接的双方
Attribute：收集到的信息，比如Anomaly、metric和log
系统有以下几个modules：
anomalous region module Extractor从system graph（系统的什么图？）中抽取一个子图，也就是anomalous region的子图。对于计算型的资源，选择异常节点周围2跳的所有节点作为子图，见图4。其他情况没提到233
pattern module 已经被标记为异常的节点，会作为计算相似度的模板。目的是为了对经常出现的异常做一个初始的集合，来避免冷启动的问题。（这里的冷启动是指异常区域没有可以对比相似度的对象）
similarity module 输入：1. 异常区域（anomaly region） 2. 异常模版（pattern）
输出：1. 相似度得分 2. 节点对应图（见图5）
4 Graph Similarity 都是自定义的相似度计算方法，包括两个图的相似度、两个节点的相似度、节点的属性相似度。
5 Monitoring and Building Graphs Prom做监控，cadvisor和node expoter作为监控agent。
构建图的过程其实是节点和节点连接的过程，TCP连接就可以构建一个边，由此创建调用图。
6 Evaluation 场景 不同的场景有不同的异常注入方式
 面向服务的场景 负载均衡场景 Kafka集群场景 Spark&amp;amp;HDFS场景  异常注入方式  stress做cpu、mem压力异常 连接异常（带宽限制） 负载均衡异常 高并发异常  Precision Training set的构建方式：异常注入的情况可以得到一些region啊，比如我用stress做异常测试，得到微服务m1出现异常，然后把m1周围2跳的节点抽取出来作为region。这就是一个新的pattern。</description>
    </item>
    
    <item>
      <title>Microscope</title>
      <link>https://zecoo.github.io/hugo/posts/paper/microscope-pinpoint-performance-issues-with-causal-graphs-in-micro-service-environments/</link>
      <pubDate>Wed, 20 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://zecoo.github.io/hugo/posts/paper/microscope-pinpoint-performance-issues-with-causal-graphs-in-micro-service-environments/</guid>
      <description>Microscope: Pinpoint Performance Issues with Causal Graphs in Micro-service Environments
ICSOC 2018
Abstruct 错误定位的论文。通过服务调用关系构建DAG定位错误。
Introduction 定位有88%的准确性，2020的icws有一篇错误定位论文跟该论文进行过比较。
benchmark用的是sock shop
System Overview 三个步骤
 data collection 收集的是两个服务之间的网络连接和SLO metric（应该和SLA差不多） casual graph building 用收集到的数据构建一个casual graph（服务调用关系图？） rank cause inference 根据graph，列出所有root cause候选并排序  System design data collection 需要从“127.0.0.1:port -&amp;gt; 172.80.12.98:port”的调用关系map到“Service A instance1 -&amp;gt; Service B instance 1”
SLO可以从服务监控软件中获取到服务响应时间
casual graph building 如果是Service A到Service B的错误定位，1中已经提到了。
还有一种情况是Service内部instance和instance之间的关系，如果有一个instance占据了所有的cpu利用率，同一个service内的其他instance就不能和其他服务进行通信。本文采用PC-Algorithm构建DAG，一种用概率的方式来解决该问题。（细节需要去看PC-Algorithm的论文）
rank cause inference 通过一个异常的节点（前端报出的异常节点），遍历该节点下的所有子节点，然后把所有的异常子节点都加入到候选集中。
那么如何判断一个节点是否是异常节点？
3sigma interval规则，即正态分布的3sigma，如果一个node的metric落在了99.7%的范围之外，就认为其是abnormal。3sigma interval是多少呢？（后面提了一嘴：“The sample interval in service request latency metrics is 1s.</description>
    </item>
    
    <item>
      <title>RCA in multitier service</title>
      <link>https://zecoo.github.io/hugo/posts/paper/rca-in-multitier-service/</link>
      <pubDate>Wed, 20 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://zecoo.github.io/hugo/posts/paper/rca-in-multitier-service/</guid>
      <description>Root Cause Analysis of Anomalies of Multitier Services in Public Clouds
TON 2018
读完感觉不像是A类的期刊，Chinglish的情况很严重，相似度的计算实在是不优美，最后baseline挑的也很奇怪。
IBR 这篇文章里用了一个很贴切的词：tenant，RCA用另一种解释其实就是tenant，也就是之前网易的面试官跟我说的profiling，是因为一个节点Root出现故障，导致和该节点有关的所有节点都变慢了。
这篇也是在前面会议论文的基础上加了一些内容投的期刊。
System Arch Anomaly Type 内在因素：服务调用过程中出现的异常
外在因素：在同一个物理机上的异常导致该物理机上不同的VM的异常
System 两部分，第一部分构建异常传播图（也是一个DAG），第二部分计算相似度Rank异常节点。
Graph VCG（VM Communication Graph）用工具PreciseTracer可以获得。
APG（Anomaly Propagation Graph）
Root Cause Location 这里面定义的相关度和其他不太一样，Sim(VMi, Rj)，是计算某个VM和某个请求之间的相关度。
Data Collection  服务响应时间，这里的计算方式是将所有参与一次响应的相关组件的响应时间相加 Utilization，比如CPU、mem等  都是老生常谈了
Similarity Calculation 依旧是皮尔逊相关系数，不过这个相关度也太复杂了吧？
R(i, M, R&amp;reg;, ts, te) = cov(Mte(M,i),Tte (R&amp;reg;)􏲊 / σMte (M,i)σTte (R&amp;reg;)
Random Walk 依旧是Forward、Backward和Self，就是随机游走哪一套
Evaluation Baseline  RS：Random Select，普通人用排列组合的方式选择 SC：Sudden Change，比较当前metric和一个时间窗口之前的metric的变化，变化最大的那个视为Anomaly DBR：Distance Based Rank，选择最优传播路径，找传播过程花费距离最短的  Evaluation Metric</description>
    </item>
    
    <item>
      <title>MS-Rank</title>
      <link>https://zecoo.github.io/hugo/posts/paper/ms-rank/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://zecoo.github.io/hugo/posts/paper/ms-rank/</guid>
      <description>MS-Rank: Multi-Metric and Self-Adaptive Root Cause Diagnosis for Microservice Applications
ICWS 2019
主要贡献：  根据一系列的metrics，构建一个调用图
 基于随机游走算法提出了根因定位算法
 为了提高定位的准确性，用一种自适应的方法调整metrics的权重
  2 RW 提到了Microscope和Sieve
论文里居然还提到了SRE…hh
3 Solution 3A Framwork 针对不同的情景，选择的metric也不一样。UI相关的，一般更在意latency；计算相关的比如hadoop，更在意cpu负载。文章把微服务系统看成是一个黑盒，那么metric的权重这样的概念就出来了。
3B Metric latency适合短连接服务，但不适合长连接服务。
常见的metric：latency、throughput、CPU、MEM、disk、I/O，power。power = throughput / latency
Gradient相关重要的metric：
Gradient(t) = Mj,i(t) - Mji(t-1)
Gradient ratio(t) = Gradient(t) / Mj,i(t)
还有两个没列出来
3C Graph Construction 基于PC算法，依旧是一个DAG。额，这部分看得我一头雾水
3D Random Walk Diagnosis 一个最直观也是最常用的根因定位方法是计算某个服务的metric和Anomaly服务的metric的相似度。我看的其他论文里也大致是这个思想
同样这篇论文用皮尔逊相似度，不过添加了权重的部分。可以得到vi和vj的关系（相似度）
Transition（游走）：向前（遇到高相似度）、向内（前后都是低相似度，则停留在自身）、向后（遇到低相似度）
3E Weight Update P(M) Precision Function，计算定位结果的准确性。还有更新权重，都是一个公式带过。
3F Example 收集了2小时（7200s）的数据，每5s一次收集，对于每个metric都有1440个记录。</description>
    </item>
    
    <item>
      <title>RCA Papers</title>
      <link>https://zecoo.github.io/hugo/posts/paper/rca-papers/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0800</pubDate>
      
      <guid>https://zecoo.github.io/hugo/posts/paper/rca-papers/</guid>
      <description>Literature Survey: Root Cause Analysis in Microservice System 摘要 微服务系统在最近几年迅速发展，微服务系统由多个独立部署并相互协作的微服务组成，大型的微服务系统可能包含几十个微服务，使得他们之间的调用链非常复杂。当某个微服务节点发生异常，可能会影响整个微服务调用链。为了保证Qos，定位发生异常的微服务节点显得尤为重要。目前RCA的过程大致可以分为服务调用链构建、筛选候选异常节点、排序异常节点这三个部分。这篇论文，我们将对比近几年RCA的发展以及常见的错误定位方法及这些方法之间的对比。基于以上讨论，得出对RCA的总结。
Microservice systems have developed rapidly in recent years. Microservice systems consist of multiple microservices which are independently deployed and co- ordinated . A large microservice system may contain dozens of microservices, making the call chain among these microservices very complicated. When an anomaly occurs in a microservice node, it may affect the entire call chain. In order to ensure Qos(Qunatity of service) and SLA(Service Level Assurance), it is particularly important to locate the abnormal microservice nodes.</description>
    </item>
    
  </channel>
</rss>