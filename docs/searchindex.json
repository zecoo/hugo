{"categories":[{"title":"Categories","uri":"https://zecoo.github.io/hugo/categories/"},{"title":"blog","uri":"https://zecoo.github.io/hugo/categories/blog/"},{"title":"css","uri":"https://zecoo.github.io/hugo/categories/css/"},{"title":"hello-world","uri":"https://zecoo.github.io/hugo/categories/hello-world/"},{"title":"hugo","uri":"https://zecoo.github.io/hugo/categories/hugo/"},{"title":"istio","uri":"https://zecoo.github.io/hugo/categories/istio/"},{"title":"js","uri":"https://zecoo.github.io/hugo/categories/js/"},{"title":"k8s","uri":"https://zecoo.github.io/hugo/categories/k8s/"},{"title":"math","uri":"https://zecoo.github.io/hugo/categories/math/"},{"title":"paper","uri":"https://zecoo.github.io/hugo/categories/paper/"},{"title":"prom","uri":"https://zecoo.github.io/hugo/categories/prom/"},{"title":"server","uri":"https://zecoo.github.io/hugo/categories/server/"}],"posts":[{"content":" 场景驱动、自底向上的单体系统微服务拆分方法\n评价 核心思想：把数据表之间紧密联系的聚在一起，作为拆分微服务的准则。\n实际上是关于数据表的划分，微服务的其他方面比如class、method的划分没有提到。\nAIBR 结合Scenario、Trace和Sql进行微服务拆分。\n单体系统的微服务化拆分也可以借鉴传统软件模块化的思路和方法,已有不少这方面的研究\n获取系统运行数据的工具：Kieker\n拆分方法 图1是整个流程图，需要搞清楚几个点\n 共享群组是什么 数据表权重图代表什么  图3也很关键，一个scenario数据访问的trace。\n同样还有图4，数据库的相关度graph\n数据表关联度 表关联度高表示表A和表B在一个场景要同时使用的情况比较多。比如说注册要用到user表和profile表。登录也要用到这两个表。就说明user表和profile表的关联度较高。\n sql的权重 两个数据表之间的关联度  表A和表B的关联度 = Cscenario + Ctrace + Csql\nCscenario = 同时操作表A+表B的场景之和 / 操作表A或表B中任意一张表的场景之和，trace、sql同理\n共享群组 共享度高表示一张表被多个场景使用，更倾向于单独拆分成一个微服务。比如account表在注册场景、下单场景都用到了，说明account表的共享度较高。\n共享度 = Sscenario + Strace + Ssql\nSscenario = 操作表t的场景数量 / 总场景数量，trace、sql同理\n共享表的数量 = 共享表占比 * 所有表的数量\n那么共享表占比是如何计算出来的呢？（没有提，可能是共享度达到某个值以上的就算是共享表吧）\n表依赖度 和关联度、共享度类似，表示表A对表B的依赖程度。那和关联度有什么区别呢…依赖度高，表A和表B更有可能有一种主从关系。见图6\nDscenario = 同时操作表A和表B的场景 / 操作表A的场景之和（Tb对Ta的依赖）\n共享群组条件 Ta和Tb的相互依赖程度（Ta -\u0026gt; Tb + Tb -\u0026gt; Ta）大于某一个值\n将x张共享表划分为r个共享群组\n数据表权重矩阵 有n张表，就能构建n*n的数据表关联矩阵。\n共享群组的意义在于，将同一个共享群组内的表关联度权重加强，不在同一个共享群组的数据表关联度减弱。\n数据表图聚类 上一步的权重矩阵作为聚类的图输入，GN聚类算法是一种社区发现算法，认为连接两个社区的边有更低的权重，将这些边删除，剩下的网络就被划分为独立的社区。见图4。\nbetweenness：经过某条边的所有最短路金的数量。重复计算Betweeness和删除Betweeness的过程。\nModularity 那么聚类到什么程度？引入了模块度Modularity的概念，模块度表示一个网络的紧密程度。 $$ Modularity = \\sum_i^c (eii - ai^2) $$ eii表示社区i内所有边的权重占整个网络边权重的比例，ai表示与社区i内定点相连的所有边的权重占整个网络所有顶点所连边的权重的比例。\n模块度的取值范围是[-0.5,1)，值越大说明网络内的聚类特性越明显。\n确定拆分方案 拆分开销 Cost $$ Cost = µ1 * Vsql + µ2 * Vclass + µ3 * Vmethod $$\nVsql表示需要拆分sql的数量。µ表示权重。class和method的如何拆分没有介绍呀\n推荐方案倾向于减小拆分开销。\n确定拆分方案 对于每一个拆分方案Pi有： $$ Score = w1 * modularity(Pi) + w2 * cost(Pi) （其中w1+w2=1） $$ 得分最高的作为拆分方案\n反馈调整 用户可以对得到的拆分方案进行调整\n工具实现 直接看论文里的图吧。\n疑问 Q 数据表是什么\nA 数据表权重图的每个节点表示一个数据表，每条边表示数据表和数据表之间的关联度\nQ Method、Sql和Table之间的关系\nA 见图3\nQ 数据表权重图有更直观的例子吗\nA 见图4\n","id":0,"section":"posts","summary":"场景驱动、自底向上的单体系统微服务拆分方法 评价 核心思想：把数据表之间紧密联系的聚在一起，作为拆分微服务的准则。 实际上是关于数据表的划分，微服","tags":["decomposition"],"title":"Scenario Driven MS Decomposition","uri":"https://zecoo.github.io/hugo/2020/05/scenario-driven-ms-decomposition/","year":"2020"},{"content":" 参考 https://www.jianshu.com/p/7863fcb34aed\n","id":1,"section":"posts","summary":"参考 https://www.jianshu.com/p/7863fcb34aed","tags":["blog"],"title":"vps搭建图床","uri":"https://zecoo.github.io/hugo/2020/05/vps%E6%90%AD%E5%BB%BA%E5%9B%BE%E5%BA%8A/","year":"2020"},{"content":" a triangular tract of sediment deposited at the mouth of a river, typically where it diverges into several outlets.\u2028 the fourth letter of the Greek alphabet ( Δ , δ ), transliterated as ‘d.’.\u2028 variation of a variable or function.\u2028 一组影响系统执行dimensions，高效找出导致错误的deltas。这里delta包括（deployment、enviromental、configurations）\u2028 四个dimensions：node、instance、configuration、sequence\u2028 node：分布式的服务器，未知性较多\u2028 instance：微服务的instance一般都是有状态的，如果没有处理好，相同的instance如果不是同样的状态就会出错。\u2028 configuration：docker的配置，k8s的配置，是挺恶心的，万一内存不够，就爆了\u2028 sequence：异步的invocation导致错误\u2028 微服务settings作为circumstances，就可以用delta bebugging了，db是一种简化和隔离错误案例的方法\u2028 Istio是微服务service mesh的？Istio可以部署在k8s里的\u2028 delta debugging的作用是找到min的deltas导致错误\u2028 controller：delta debugging的位置，是文章核心嗷\u2028 scheduler：根据容器状态选择test case去执行。queue，有可用资源就去跑测试\u2028 executor：根据所给的circumstances在docker上跑测试用例，返回测试结果\u2028 circumstance：4个dimension的组合\u2028 delta：两个cir之间的差异\u2028 delta db目的：通过最简单cir抽取导致错误的最小delta set\u2028 min setting：一个node，一个instance，默认conf（full memory），正确的seq\u2028 general setting：一个可能引起错误的cir\u2028 表示方法：0表示min setting，1表示general setting\u2028 seq的表示方法稍微复杂一点，0表示顺序执行，1表示逆序执行\u2028 seq能全覆盖，node、instance numbers可以放低一点要求\u2028 If the given failing test case still fails with the simplest circumstance, the failure can be thought to be caused by internal faults of related microservices\u2028 我们的目标是找到应用在min cir上导致ftc产生ftr的同时ptc产生ptr的deltas\u2028 atomic delta是什么意思？？\u2028 we partition the set of deltas X into n equal-sized partitions\u2028 13vm each 8-core cpu 24gb memory\u2028 36-63 deltas -》 1-2deltas\u2028 就是能分出到底是哪种问题\u2028 但是有个很重要的点就是要开发再确认bug\u2028  ","id":2,"section":"posts","summary":"a triangular tract of sediment deposited at the mouth of a river, typically where it diverges into several outlets. the fourth letter of the Greek alphabet ( Δ , δ ), transliterated as ‘d.’. variation of a variable or function. 一组影响系统执行dimensions，高效找出导致错","tags":["RCA"],"title":"Delta debug MS","uri":"https://zecoo.github.io/hugo/2020/05/delta-debug-ms/","year":"2020"},{"content":" Graph-based root cause analysis for service-oriented and microservice architectures\nJSS 2020\n2 RW RCA方法有以下大致分类\n model-based的RCA方法 Spectrum（也可以称之为Classification）方法。有人用ML来做 图的方法  2017年的Sieve，MS-Rank也提到了这篇论文。\n3 Graph-based RCA 举一个Wordpress的例子，如果不做loadbalance，HAProxy的运行缓慢可能是其中某一个服务器负载过大。\nGraph Modules Node：\nEdge：网络连接的对象，例如一次TCP连接的双方\nAttribute：收集到的信息，比如Anomaly、metric和log\n系统有以下几个modules：\nanomalous region module Extractor从system graph（系统的什么图？）中抽取一个子图，也就是anomalous region的子图。对于计算型的资源，选择异常节点周围2跳的所有节点作为子图，见图4。其他情况没提到233\npattern module 已经被标记为异常的节点，会作为计算相似度的模板。目的是为了对经常出现的异常做一个初始的集合，来避免冷启动的问题。（这里的冷启动是指异常区域没有可以对比相似度的对象）\nsimilarity module 输入：1. 异常区域（anomaly region） 2. 异常模版（pattern）\n输出：1. 相似度得分 2. 节点对应图（见图5）\n4 Graph Similarity 都是自定义的相似度计算方法，包括两个图的相似度、两个节点的相似度、节点的属性相似度。\n5 Monitoring and Building Graphs Prom做监控，cadvisor和node expoter作为监控agent。\n构建图的过程其实是节点和节点连接的过程，TCP连接就可以构建一个边，由此创建调用图。\n6 Evaluation 场景 不同的场景有不同的异常注入方式\n 面向服务的场景 负载均衡场景 Kafka集群场景 Spark\u0026amp;HDFS场景  异常注入方式  stress做cpu、mem压力异常 连接异常（带宽限制） 负载均衡异常 高并发异常  Precision Training set的构建方式：异常注入的情况可以得到一些region啊，比如我用stress做异常测试，得到微服务m1出现异常，然后把m1周围2跳的节点抽取出来作为region。这就是一个新的pattern。\nthreshold是控制不同pattern的参数，threshold设为0.9表示和训练集中已有的pattern的相似度小于90%的region设为一个新的pattern。\n实验做两次，第一次不做参数调整，第二次做参数调整。\n好好看一下table2，挺有意思的。拿第一列来说，Kafka场景下，比如我用CPU做异常注入，不做参数调整得到的CPU pattern的相似度是0.43，和Disk、Band、Heap pattern的相似度分别是0.18、0.03、0.34。但是用参数调整得到的CPU相似度是0.87。\n那么参数调整具体怎么做的呢？\n一是领域专家做调整，二是根据节点自身附带的其他信息，比如这个节点本身是cpu联系性较高的，那就把CPU pattern的参数调高。\n图15表示计算region和pattern之间相似度的时间会随着节点的增多而变大。\n","id":3,"section":"posts","summary":"Graph-based root cause analysis for service-oriented and microservice architectures JSS 2020 2 RW RCA方法有以下大致分类 model-based的RCA方法 Spectrum（也可以称之为Classificatio","tags":["RCA"],"title":"Graph-based RCA in SOA and MS","uri":"https://zecoo.github.io/hugo/2020/05/graph-based-rca-in-soa-and-ms/","year":"2020"},{"content":" Microscope: Pinpoint Performance Issues with Causal Graphs in Micro-service Environments\nICSOC 2018\nAbstruct 错误定位的论文。通过服务调用关系构建DAG定位错误。\nIntroduction 定位有88%的准确性，2020的icws有一篇错误定位论文跟该论文进行过比较。\nbenchmark用的是sock shop\nSystem Overview 三个步骤\n data collection 收集的是两个服务之间的网络连接和SLO metric（应该和SLA差不多） casual graph building 用收集到的数据构建一个casual graph（服务调用关系图？） rank cause inference 根据graph，列出所有root cause候选并排序  System design data collection 需要从“127.0.0.1:port -\u0026gt; 172.80.12.98:port”的调用关系map到“Service A instance1 -\u0026gt; Service B instance 1”\nSLO可以从服务监控软件中获取到服务响应时间\ncasual graph building 如果是Service A到Service B的错误定位，1中已经提到了。\n还有一种情况是Service内部instance和instance之间的关系，如果有一个instance占据了所有的cpu利用率，同一个service内的其他instance就不能和其他服务进行通信。本文采用PC-Algorithm构建DAG，一种用概率的方式来解决该问题。（细节需要去看PC-Algorithm的论文）\nrank cause inference 通过一个异常的节点（前端报出的异常节点），遍历该节点下的所有子节点，然后把所有的异常子节点都加入到候选集中。\n那么如何判断一个节点是否是异常节点？\n3sigma interval规则，即正态分布的3sigma，如果一个node的metric落在了99.7%的范围之外，就认为其是abnormal。3sigma interval是多少呢？（后面提了一嘴：“The sample interval in service request latency metrics is 1s.”）不知道是否可以理解为一个node的响应时间超过1*99.7%秒就认定其为异常节点。\n用皮尔逊相关系数来做rank，将候选集中的节点和初始的前端异常进行相关度计算，相关度最高的rank最高。\nExperiment 4台物理机，64G Memory Ubuntu16\n数据收集部分，先把数据存成log，然后用了两个工具：Filebeat集中日志，elasticsearch保存Filebeat收集到的数据。这应该是ELTK那一套。\n用Prometheus工具获取service的request latency metrics\nsock-shop支持模拟大规模用户请求，这里QPS（Query Per Seconds）可以模拟到5k\nSock-Shop还有istio、Prometheus都要摸熟\nFault Injection 模拟错误，3种类型：\n CPU满载\n Traffic Jam，用linux工具模拟\n Container Pause\n  实验做得相当饱满\n Sock-Shop不同的服务对上面提到的3种错误模拟的敏感程度\n 对比其他方法7种方法的precision\n Rank X能命中错误？对比各种方法的Rank1-10的准确率\n Scalability：如果instance从36个增加到120个，对准确率有影响吗？仅有7%的降低。\n  ","id":4,"section":"posts","summary":"Microscope: Pinpoint Performance Issues with Causal Graphs in Micro-service Environments ICSOC 2018 Abstruct 错误定位的论文。通过服务调用关系构建DAG定位错误。 Introduction 定位有88%的准确性，2020的icws有一篇错误定位论文跟","tags":["RCA"],"title":"Microscope","uri":"https://zecoo.github.io/hugo/2020/05/microscope-pinpoint-performance-issues-with-causal-graphs-in-micro-service-environments/","year":"2020"},{"content":" Root Cause Analysis of Anomalies of Multitier Services in Public Clouds\nTON 2018\n读完感觉不像是A类的期刊，Chinglish的情况很严重，相似度的计算实在是不优美，最后baseline挑的也很奇怪。\nIBR 这篇文章里用了一个很贴切的词：tenant，RCA用另一种解释其实就是tenant，也就是之前网易的面试官跟我说的profiling，是因为一个节点Root出现故障，导致和该节点有关的所有节点都变慢了。\n这篇也是在前面会议论文的基础上加了一些内容投的期刊。\nSystem Arch Anomaly Type 内在因素：服务调用过程中出现的异常\n外在因素：在同一个物理机上的异常导致该物理机上不同的VM的异常\nSystem 两部分，第一部分构建异常传播图（也是一个DAG），第二部分计算相似度Rank异常节点。\nGraph VCG（VM Communication Graph）用工具PreciseTracer可以获得。\nAPG（Anomaly Propagation Graph）\nRoot Cause Location 这里面定义的相关度和其他不太一样，Sim(VMi, Rj)，是计算某个VM和某个请求之间的相关度。\nData Collection  服务响应时间，这里的计算方式是将所有参与一次响应的相关组件的响应时间相加 Utilization，比如CPU、mem等  都是老生常谈了\nSimilarity Calculation 依旧是皮尔逊相关系数，不过这个相关度也太复杂了吧？\nR(i, M, R\u0026reg;, ts, te) = cov(Mte(M,i),Tte (R\u0026reg;)􏲊 / σMte (M,i)σTte (R\u0026reg;)\nRandom Walk 依旧是Forward、Backward和Self，就是随机游走哪一套\nEvaluation Baseline  RS：Random Select，普通人用排列组合的方式选择 SC：Sudden Change，比较当前metric和一个时间窗口之前的metric的变化，变化最大的那个视为Anomaly DBR：Distance Based Rank，选择最优传播路径，找传播过程花费距离最短的  Evaluation Metric\n PR@K（Precision at TOP K）TOP K的准确性。比如K=5，能有90%的准确性Root Cause在里面 MAP（Mean Average Precision）  ","id":5,"section":"posts","summary":"Root Cause Analysis of Anomalies of Multitier Services in Public Clouds TON 2018 读完感觉不像是A类的期刊，Chinglish的情况很严重，相似度的计算实在是不优美，最后baseline挑的也很奇","tags":["RCA"],"title":"RCA in multitier service","uri":"https://zecoo.github.io/hugo/2020/05/rca-in-multitier-service/","year":"2020"},{"content":" MS-Rank: Multi-Metric and Self-Adaptive Root Cause Diagnosis for Microservice Applications\nICWS 2019\n主要贡献：  根据一系列的metrics，构建一个调用图\n 基于随机游走算法提出了根因定位算法\n 为了提高定位的准确性，用一种自适应的方法调整metrics的权重\n  2 RW 提到了Microscope和Sieve\n论文里居然还提到了SRE…hh\n3 Solution 3A Framwork 针对不同的情景，选择的metric也不一样。UI相关的，一般更在意latency；计算相关的比如hadoop，更在意cpu负载。文章把微服务系统看成是一个黑盒，那么metric的权重这样的概念就出来了。\n3B Metric latency适合短连接服务，但不适合长连接服务。\n常见的metric：latency、throughput、CPU、MEM、disk、I/O，power。power = throughput / latency\nGradient相关重要的metric：\nGradient(t) = Mj,i(t) - Mji(t-1)\nGradient ratio(t) = Gradient(t) / Mj,i(t)\n还有两个没列出来\n3C Graph Construction 基于PC算法，依旧是一个DAG。额，这部分看得我一头雾水\n3D Random Walk Diagnosis 一个最直观也是最常用的根因定位方法是计算某个服务的metric和Anomaly服务的metric的相似度。我看的其他论文里也大致是这个思想\n同样这篇论文用皮尔逊相似度，不过添加了权重的部分。可以得到vi和vj的关系（相似度）\nTransition（游走）：向前（遇到高相似度）、向内（前后都是低相似度，则停留在自身）、向后（遇到低相似度）\n3E Weight Update P(M) Precision Function，计算定位结果的准确性。还有更新权重，都是一个公式带过。\n3F Example 收集了2小时（7200s）的数据，每5s一次收集，对于每个metric都有1440个记录。\n示例的这个图画得挺好的，体现出了不同的metric在trace中的表现。但是这图看不懂原理啊\n4 Evaluation 4.1 Benchmark 有两个小问题：1. 没有考虑数据库 2. 并不是真正意义上的ms，是把一个api视为一个ms。所以没有用到k8s等\nBenchmark：TBAC、MonitorRank、NetMedic和Gestalt我只听说过MR\n在一台16G Windows Server上进行实验\n比较了Precision、Adaptability和domain knowledge（领域知识能增强准确性）\n最后还有不同的参数对实验的影响。\n5 Final 他们和Pengfei Chen是一个圈子里的啊，中山大学和清华深研院应该有交易的hh。我感觉我们不在这个圈子里啊。\n","id":6,"section":"posts","summary":"MS-Rank: Multi-Metric and Self-Adaptive Root Cause Diagnosis for Microservice Applications ICWS 2019 主要贡献： 根据一系列的metrics，构建一个调用图 基于随机游走算法提出了根因定位算法 为了提高定位的准确性，用一种自","tags":["icws"],"title":"MS-Rank","uri":"https://zecoo.github.io/hugo/2020/05/ms-rank/","year":"2020"},{"content":" Literature Survey: Root Cause Analysis in Microservice System 摘要 微服务系统在最近几年迅速发展，微服务系统由多个独立部署并相互协作的微服务组成，大型的微服务系统可能包含几十个微服务，使得他们之间的调用链非常复杂。当某个微服务节点发生异常，可能会影响整个微服务调用链。为了保证Qos，定位发生异常的微服务节点显得尤为重要。目前RCA的过程大致可以分为服务调用链构建、筛选候选异常节点、排序异常节点这三个部分。这篇论文，我们将对比近几年RCA的发展以及常见的错误定位方法及这些方法之间的对比。基于以上讨论，得出对RCA的总结。\nMicroservice systems have developed rapidly in recent years. Microservice systems consist of multiple microservices which are independently deployed and co- ordinated . A large microservice system may contain dozens of microservices, making the call chain among these microservices very complicated. When an anomaly occurs in a microservice node, it may affect the entire call chain. In order to ensure Qos(Qunatity of service) and SLA(Service Level Assurance), it is particularly important to locate the abnormal microservice nodes. The anomaly detection process is called root cause analysis. At present, the process of root cause analysis can be roughly divided into three parts: service call graph construction, listing of candidate abnormal nodes, and ranking abnormal nodes to find out the anomaly. In this paper, we will discuss the development of root cause analysis in recent years, as well as common anomaly localization methods and the comparison between these methods. Finally, based on the above discussion, we will give a breif summry about root cause analysis.\n关键词：root cause， microservice，anomaly detection\nIntro 传统的单体架构出现异常，寻找异常解决问题的过程需要花费大量的时间成本和人力成本。随着单体架构系统的继续扩大，排除异常的成本也会越来越高。于是微服务系统越来越受到工业界的认可，将大型的单体架构拆分成多个可以独立部署，通过REST相互协作的微服务，大大降低了运维的复杂度。\n但同时也带来了一些问题，由于系统的微服务数量较多，且微服务之间相互协作的复杂度较高，如果其中一个微服务出现异常，会牵连到与其协作的微服务均出现不同程度的异常。可能是服务响应时间变高，甚至有可能是某个调用过程卡住无法向前传递。RCA就是为了解决这个问题，快速定位出现异常的节点。\n这篇文章我们将粗略了解一下RCA的流程以及当前比较流行的异常探测方法。文章讲按照以下部分进行：\nThe abnormality of the traditional monolithic architecture, the process of finding the abnormal solution to the problem requires a lot of time cost and labor cost. As the monolithic architecture continues to expand, the cost of eliminating anomalies will also increase. As a result, the microservice system is more and more recognized by the industry. The large-scale monolithic architecture is split into multiple microservices that can be independently deployed and cooperate with each other through REST, which greatly reduces the complexity of operation and maintenance.\nBut at the same time, it also brings some problems. Due to the large number of microservices in the system and the high complexity of cooperation between microservices, if one of the microservices is abnormal, the microservices with which it cooperates will appear different Abnormal degree. It may be that the service response time becomes higher, or it may even be that a certain calling process is stuck and cannot be forwarded. RCA is to solve this problem and quickly locate abnormal nodes.\nIn this article, we will take a rough look at the process of RCA and the currently popular anomaly detection methods. The article follows the following sections:\n问题\u0026amp;挑战  复杂的网络依赖性。通过微服务架构，应用程序被分解为具有异常复杂的网络拓扑的许多细粒度的组件。而且，为了连接包裹在容器中的不同微服务，总是采用覆盖网络的方法，进一步增加了性能诊断的复杂性。 动态运行时环境。微服务系统通常在容器状态经常变化的容器化环境中运行。高度动态的环境加剧了性能诊断的难度。 大量的监视指标。由于微服务系统中共存有许多服务，因此这些服务的监视指标（例如响应时间）的数量非常大。指出Netflix，Uber和OpenStack分别有2,000,000个指标，500,000,000个指标和17608个指标需要监视。如何从这些数据中找出根本原因是一个具有挑战性的问题。  -Complex networks allow. Through the microservices architecture, applications are broken down into many fine-grained components with unusually complex network topologies. Moreover, in order to connect different microservices wrapped in containers, the method of overlay network is always adopted. , Further increasing the complexity of performance diagnosis. -Dynamic runtime environment. Microservice systems usually operate in a containerized environment where the container state changes frequently. The highly dynamic environment exacerbates the difficulty of performance diagnosis. -A large number of monitoring indicators. Since there are many services coexisting in the microservice system, the number of monitoring indicators (such as response time) of these services is very large. Point out that Netflix, Uber and OpenStack have 2,000,000 indicators, 500,000,000 indicators and 17,608 indicators need to be monitored. How to find the root cause from these data is a challenging problem.\nSLO Metrics Low-level metrics Low-level metrics in the context of this survey, are server information monitored at the physical server/virtual machine layer by hypervisors, such as utilization of CPU, memory, and network resources, memory swap, and cache miss rate.\nHigh-level metrics High-level metricsare performance indicators observed at the application layer. Those useful to auto-scaling include resource rate, average response time, session creation rate, throughput, service time, and request mix.\nWorkflow Data Collection 根据以上提到metric类型的不同，采集的方式也不尽相同。对于low level的metric采集，一般是实时采集，当前某时刻的cpu负载和内存、磁盘利用率。对于high level的metric采集过程，分为主动采集和被动采集。现在比较流行的是被动采集方法，让应用程序暴露一个pull metric的接口，然后使用数据采集工具定时将数据拉取下来。采集到这些数据之后存入时序数据库中，方便调用和查看。根据【】中提到的，采集到的数据一般包含时间戳、通信双方的ip、通信的响应时间、响应状态码以及通信类型等。\n27092018 09:42:15 UTC, GET, /test/users/1, 172.18.0.10 , 57170, 172.18.0.6 ,8888, 200, 2.31 , RequestResponse  这样基础的日志分析一般是对日志文件的分析，不够直观，于是有了一些工具帮助我们更好发现异常，【】提到一些链路可视化工具比如Zipkin等，可以将调用链以及在某个请求处所花费的时间直观表现出来。\nGraph Building 在采集到这样的原始数据之后，需要将ip对应关系映射到某个微服务的具体容器，基于以上数据构建服务调用图。在微服务架构之前，流行的架构模式是SOA，multitier service是其中一种架构。【】讨论了在multitier service系统下，服务调用图将跨越不同的VM，经由VM上的不同应用完成整个请求调用过程。一个VM上可能有多个服务，如果VM出现异常，那么在这个VM上的所有服务都会收到影响。\n相同的思想，【】关注到在微服务领域，一台物理机上的不同容器，会受到物理机的影响。故将依赖类型分为协作依赖和非协作依赖。\n同时我们定义：\nAfter collecting such raw data, you need to map the IP correspondence to a specific container of a microservice, and build a service call graph based on the above data. Before the microservice architecture, the popular architectural pattern was SOA, and multitier service was one of them. [] It is discussed that under the multitier service system, the service call graph will span different VMs and complete the entire request call process through different applications on the VM.\nCause Inference 一般异常都是前端的节点最先发现，然后经由调用图追溯到正确的异常节点。\n计算相似度 为找到候选集，我们基于收集到的metric，计算初始异常节点（通常是前端节点）和其他节点的相似性。相似性越高，我们认为其更有可能是root cause。大部分的相似度计算方法都是用皮尔逊相关系数来实现的。\nRank 这个过程出现了一些不同的思路。【】选择找到一些候选的root cause，为提高准确性对这些root cause进行排序。使用的是遍历子树的方法，如果一个节点有异常，那么遍历和其有关联的子节点，将出现异常的子节点加入候选集中。\n【】使用的是随机游走算法，认定在某个节点上停留的时间越久，那么这个节点更有可能是root cause。PageRank思想最早运用在网页领域，在互联网上，如果一个网页被很多其他网页所链接，说明它受到普遍的承认和信赖，那么它的排名就高。类似的，如果很多个节点收到某一个节点的影响，那么其是root cause的可能性就越大。\n而【】提供了一种更有趣的思路，该工作首先通过false injection创建了大量不同pattern的数据集，然后将出现的异常和数据集中的所有pattern进行相似度计算，如果匹配成功，就能根据fault injection的情况判断是出现何种异常。\nRef Microscope: Pinpoint Performance Issues with Causal Graphs in Micro-service Environments （ICSOC 2018） MS-Rank: Multi-Metric and Self-Adaptive Root Cause Diagnosis for Microservice Applications （ICWS 2019） Graph-based root cause analysis for service-oriented and microservice architectures （JSS 2020） Microservices Monitoring with Event Logs and Black Box Execution Tracing （TSC 2019） Fault Analysis and Debugging of Microservice Systems: Industrial Survey, Benchmark System, and Empirical Study （TSE 2018） Root Cause Analysis of Anomalies of Multitier Services in Public Clouds （TON 2018） Ranking Causal Anomalies by Modeling Local Propagations on Networked Systems （ICDM 2017） Fault Analysis and Debugging of Microservice Systems: Industrial Survey, Benchmark System, and Empirical Study （TSE 2018）  感想 How did you choose your reference papers? 我的想法是不要把6篇参考文章都选为RCA相关的。\n文献6详细介绍了当前微服务系统下的日志分析现状，从传统的日志分析到可视化日志分析到可视化链路日志分析，对日志的基本情况可以做一个大致的把握。\n文献5深入日志，还原了日志最原始的数据展示情况。可以更清楚了解到日志的组成部分。\n文献1-4代表现阶段一些不同的RCA研究方法，有构造异常模版的方法，也有构造服务调用DAG的方法。但是大致的研究思路是相同的。\nWhy did you choose the papers? 最近看了几篇论文都是RCA相关的，刚好把看的一些论文给总结一下。我问过业界的工作人员，微服务领域弹性伸缩、RCA、Decompose都是存在而且是不可分割的问题。说明这部分的研究是非常有现实意义的。\n在单体架构盛行的年代，我们更关心的是系统出错了我们如何debug，到了微服务系统中，debug也要做，但是debug的复杂性会随着decompose的程度而减弱。这样同时带来的是服务于服务之间相互协作过程中出现的异常。RCA就是为了解决这样的问题。\nWhat did you get from the final report?  latex是真的好用 写一篇论文真不是轻松的工作 尽量把工作写得详细一些  我在写的过程中总是把论文的工作用几句话就能总结出来了，但其实一些大的概念我可能只是感觉自己弄明白了。过于概括的原因其实就是没有深入细节，也就是无料可写。\n最后感觉这个作业的形式还挺有意思的，作为没有写过英语论文的我来说，算是提前体验了一把写论文的感觉。\n","id":7,"section":"posts","summary":"Literature Survey: Root Cause Analysis in Microservice System 摘要 微服务系统在最近几年迅速发展，微服务系统由多个独立部署并相互协作的微服务组成，大型的微服务系统可能包含几十个微服务，使得","tags":["valueless"],"title":"RCA Papers","uri":"https://zecoo.github.io/hugo/2020/05/rca-papers/","year":"2020"},{"content":" 我很傻的，每次更新博客之后都要手动输入\nhugo -D git add . git commit -m \u0026quot;xxx\u0026quot; git push origin master  又不想下载git客户端一键push，写个脚本咯\n#!/bin/bash hugo -D git add . echo \u0026quot;input commit info:\u0026quot; read -t 5 commit_info commit_info=${commit_info:-\u0026quot;update blog\u0026quot;} git commit -m \u0026quot;$commit_info\u0026quot; git push origin master  read提供一个默认值“update blog”，这样既可以修改commit info，5s后忘记输入的话也不用担心。\n记得最后把sh文件权限给足：chmod 777 update.sh\n参考 https://www.cnblogs.com/lottu/p/3962921.html （shell接收键盘输入）\nhttps://blog.csdn.net/u010339879/article/details/77938911 （read添加默认值）\n","id":8,"section":"posts","summary":"我很傻的，每次更新博客之后都要手动输入 hugo -D git add . git commit -m \u0026quot;xxx\u0026quot; git push origin master 又不想下载git客户端一键push，写个脚本咯 #!/bin/bash hugo -D git add . echo \u0026quot;input commit info:\u0026quot; read -t 5 commit_info commit_info=${commit_info:-\u0026quot;update","tags":["hugo","bash"],"title":"hugo上传博客脚本","uri":"https://zecoo.github.io/hugo/2020/05/hugo%E4%B8%8A%E4%BC%A0%E5%8D%9A%E5%AE%A2%E8%84%9A%E6%9C%AC/","year":"2020"},{"content":" export PS1=\u0026quot;\\[\\033[33m\\]\\u\\[\\033[0m\\]@\\[\\033[36m\\]k8s\\[\\033[0m\\]:\\[\\033[32m\\]\\W \\[\\033[0m\\]$ \u0026quot;  但是有个问题，输了命令命令行前缀是变好看了，但是一旦退出就没有效果了。要把这个设置写进文件里才行。具体是$HOME/.bashrc这个文件，这样就OK啦。\n# .bashrc # User specific aliases and functions alias rm='rm -i' alias cp='cp -i' alias mv='mv -i' # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi export PS1=\u0026quot;\\[\\033[33m\\]\\u\\[\\033[0m\\]@\\[\\033[36m\\]k8s\\[\\033[0m\\]:\\[\\033[32m\\]\\W \\[\\033[0m\\]$ \u0026quot;  vim也可以美化一下\n# vim 显示行号 cat \u0026lt;\u0026lt;EOF \u0026gt; ~/.vimrc set nu EOF  参考 https://learnku.com/articles/29209 （ssh炫酷UI）\nhttps://blog.csdn.net/yelangjueqi/article/details/45556657 （修改.bashrc文件）\n","id":9,"section":"posts","summary":"export PS1=\u0026quot;\\[\\033[33m\\]\\u\\[\\033[0m\\]@\\[\\033[36m\\]k8s\\[\\033[0m\\]:\\[\\033[32m\\]\\W \\[\\033[0m\\]$ \u0026quot; 但是有个问题，输了命令命令行前缀是变好看了，但是一旦退出就没有效果了。要把这个设置写进文件里才行。具体是$HOME/.bashrc这","tags":["server"],"title":"服务器命令行美化","uri":"https://zecoo.github.io/hugo/2020/05/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%BE%8E%E5%8C%96/","year":"2020"},{"content":"hugo new theme zik-theme  可以创建一个新的主题。自己的主题就是从这里开始。来看看初始的theme目录长什么样吧。\n├── LICENSE ├── archetypes │ └── default.md ├── layouts │ ├── 404.html │ ├── _default │ │ ├── baseof.html │ │ ├── list.html │ │ └── single.html │ ├── index.html │ └── partials │ ├── footer.html │ ├── head.html │ └── header.html ├── static │ ├── css │ └── js └── theme.toml  ","id":10,"section":"posts","summary":"hugo new theme zik-theme 可以创建一个新的主题。自己的主题就是从这里开始。来看看初始的theme目录长什么样吧。 ├── LICENSE ├── archetypes │ └── default.md ├── layouts │ ├── 404.html │","tags":["hugo"],"title":"hugo tutorial","uri":"https://zecoo.github.io/hugo/2020/05/hugo-tutorial/","year":"2020"},{"content":" 新坑哦～\n5天都没有解决的新坑哦～\n环境：  华为云学生机2c4g 鲲鹏通用计算增强型 | kc1.large.2 | 2vCPUs | 4GB  错误 kubeadm init 后卡在：\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026quot;/etc/kubernetes/manifests\u0026quot;. This can take up to 4m0s [kubelet-check] Initial timeout of 40s passed.  然后按照提示查看kubelet的日志，显示“node xxx not found”\nnode \u0026quot;ecs-kc1-large-2-linux-20200511105949\u0026quot; not found Container \u0026quot;408a5e9f9958dd3919cecf4a944a25ce30582bc638d657fc065fec8c56579f2a\u0026quot; not found in pod's containers k8s.io/kubernetes/pkg/kubelet/kubelet.go:453: Failed to list *v1.Node: Get https://192.168.0.214:6443/api/v1/nodes?fieldSelector=metadata.name%3Decs-kc1-large-2-linux-20200511105949\u0026amp;limit=500\u0026amp;resourceVersion=0: dial tcp 192.168.0.214:6443: connect: connection refused  看到最关键的了吗？dial tcp 192.168.0.214:6443: connect: connection refused，这个问题很关键，但是网上没有合适的解答。connection refused，其实就是“node not found”\n再看看docker容器的运行情况，发现k8s所有组件的容器都创建失败，处于Exited状态。kubelet就这样创建失败继续创建，陷入循环当中。\nk8s_POD_etcd\tExited (1) Less than a second ago k8s_POD_kube-controller-manager\tExited (1) Less than a second ago k8s_POD_kube-scheduler\tExited (1) Less than a second ago k8s_POD_etcd\tExited (1) Less than a second ago k8s_POD_kube-controller-manager\tExited (1) 1 second ago  售后 给华为售后也提了工单，最后得到的结果是华为鲲鹏服务器是arm架构，并且华为在上面做了自研，底层架构和amd差别还是挺大的。所以对于一些大型应用可能还没有适配。暗示k8s适配还要很久。\n这坑就不填了。俺换台服务器就是了。\n不过整体的体验还是挺好的，从提售后工单到售后邮件回复、电话回访都做得挺好的。打客服电话退订也很热心告诉我到哪里退订。\n我用了10天左右吧，手续费20，费用是5块。最后能退给我175块。还算OK，损失不大。\n","id":11,"section":"posts","summary":"新坑哦～ 5天都没有解决的新坑哦～ 环境： 华为云学生机2c4g 鲲鹏通用计算增强型 | kc1.large.2 | 2vCPUs | 4GB 错误 kubeadm init 后卡在： [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026quot;/etc/kubernetes/manifests\u0026quot;. This","tags":["k8s","kubeadm"],"title":"Kubeadm init node not found","uri":"https://zecoo.github.io/hugo/2020/05/kubeadm-init-node-not-found/","year":"2020"},{"content":" 方法：用hightlight.js做高亮。\n针对不同的Hugo主题，应该有不同的思路。但是基本思路还是：\n head里添加css body中添加两个“script“元素第一个是js文件，第二个是loadjs  我用的是Hugo的contrast主题，打开文件夹theme - layout，找到博文的base.html，打开在里面添加基本思路中的文件。我仿照代码里本来有的目录，简单修改一下就可以了。（第二行是我添加的highlight css文件）\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;{{ .Site.BaseURL }}css/index.css\u0026quot;\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;{{ .Site.BaseURL }}css/github.css\u0026quot;\u0026gt;  css的选择可以到GitHub上hightlight.js的src/style下面找。\njs文件，用官网上的cdn就可以。或者直接下载下来。\n参考 https://orianna-zzo.github.io/sci-tech/2018-01/blog养成记3-hugo的语法高亮配置/#使用highlight-shortcode进行高亮\n","id":12,"section":"posts","summary":"方法：用hightlight.js做高亮。 针对不同的Hugo主题，应该有不同的思路。但是基本思路还是： head里添加css body中添加两个","tags":["hugo","highlight"],"title":"Hugo添加代码高亮","uri":"https://zecoo.github.io/hugo/2020/05/hugo-code-highlight/","year":"2020"},{"content":" 整体思路 01 查看Prom的metric： 写好PromQL获得自己想要的metric，这时候只是文本的形式。而且又臭又长。\n\u0026quot;metric\u0026quot;: { \u0026quot;__name__\u0026quot;: \u0026quot;istio_requests_total\u0026quot;, \u0026quot;connection_security_policy\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;destination_app\u0026quot;: \u0026quot;productpage\u0026quot;, \u0026quot;destination_canonical_revision\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;destination_canonical_service\u0026quot;: \u0026quot;productpage\u0026quot;, \u0026quot;destination_principal\u0026quot;: \u0026quot;spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026quot;, \u0026quot;destination_service\u0026quot;: \u0026quot;productpage.default.svc.cluster.local\u0026quot;, \u0026quot;destination_service_name\u0026quot;: \u0026quot;productpage\u0026quot;, \u0026quot;destination_service_namespace\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;destination_version\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;destination_workload\u0026quot;: \u0026quot;productpage-v1\u0026quot;, \u0026quot;destination_workload_namespace\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;instance\u0026quot;: \u0026quot;10.244.0.61:15090\u0026quot;, \u0026quot;job\u0026quot;: \u0026quot;envoy-stats\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;istio-system\u0026quot;, \u0026quot;pod_name\u0026quot;: \u0026quot;istio-ingressgateway-6489d9556d-ws6cg\u0026quot;, \u0026quot;reporter\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;request_protocol\u0026quot;: \u0026quot;http\u0026quot;, \u0026quot;response_code\u0026quot;: \u0026quot;200\u0026quot;, \u0026quot;response_flags\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;source_app\u0026quot;: \u0026quot;istio-ingressgateway\u0026quot;, \u0026quot;source_canonical_revision\u0026quot;: \u0026quot;1.5\u0026quot;, \u0026quot;source_canonical_service\u0026quot;: \u0026quot;istio-ingressgateway\u0026quot;, \u0026quot;source_principal\u0026quot;: \u0026quot;spiffe://cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account\u0026quot;, \u0026quot;source_version\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;source_workload\u0026quot;: \u0026quot;istio-ingressgateway\u0026quot;, \u0026quot;source_workload_namespace\u0026quot;: \u0026quot;istio-system\u0026quot; }, \u0026quot;value\u0026quot;: [ 1588918429.726, \u0026quot;5148\u0026quot; ] }  02 提取metric 用代码或者在浏览器中用Prom提供的HTTP API访问提取metrics。请不要用curl，这个坑在另一个博客里已经提到了。\nhttp://prom_url/api/v1/query?query=req_total{destination_app=\u0026quot;x\u0026quot;}  03 可视化metric 最简单的方法：Grafana，我是瞎猫碰到死耗子找到的。版本v6.5.2点Explore，可以输入PromQL直接生成漂亮的graph，但是做论文的图的话可能还是要用plot画图。\n","id":13,"section":"posts","summary":"整体思路 01 查看Prom的metric： 写好PromQL获得自己想要的metric，这时候只是文本的形式。而且又臭又长。 \u0026quot;metric\u0026quot;: { \u0026quot;__name__\u0026quot;: \u0026quot;istio_requests_total\u0026quot;, \u0026quot;connection_security_policy\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;destination_app\u0026quot;: \u0026quot;productpage\u0026quot;, \u0026quot;destination_canonical_revision\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;destination_canonical_service\u0026quot;:","tags":["istio","prom"],"title":"Metric to Graph思路及实践","uri":"https://zecoo.github.io/hugo/2020/05/metric-graph%E6%80%9D%E8%B7%AF%E5%8F%8A%E5%AE%9E%E8%B7%B5/","year":"2020"},{"content":" Prom返回的json格式没有reindent，看起来很费眼睛。\n{\u0026quot;metric\u0026quot;:{\u0026quot;__name__\u0026quot;:\u0026quot;istio_requests_total\u0026quot;,\u0026quot;destination_app\u0026quot;:\u0026quot;productpage\u0026quot;,\u0026quot;destination_service\u0026quot;:\u0026quot;productpage\u0026quot;,\u0026quot;value\u0026quot;:[1588929523.247,\u0026quot;10523\u0026quot;]}]}}  在命令行，想让其用缩进好的json展示出来，就要用到jq。\ncurl http://test.json | jq  只需要在后面加上jq，就可以看到漂亮的json格式数据\n\u0026quot;metric\u0026quot;: { \u0026quot;__name__\u0026quot;: \u0026quot;istio_requests_total\u0026quot;, \u0026quot;connection_security_policy\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;destination_app\u0026quot;: \u0026quot;productpage\u0026quot;, \u0026quot;destination_canonical_revision\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;destination_canonical_service\u0026quot;: \u0026quot;productpage\u0026quot;, \u0026quot;destination_principal\u0026quot;: \u0026quot;spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026quot;, \u0026quot;destination_service\u0026quot;: \u0026quot;productpage.default.svc.cluster.local\u0026quot;, \u0026quot;destination_service_name\u0026quot;: \u0026quot;productpage\u0026quot;, \u0026quot;destination_service_namespace\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;destination_version\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;destination_workload\u0026quot;: \u0026quot;productpage-v1\u0026quot;, \u0026quot;destination_workload_namespace\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;instance\u0026quot;: \u0026quot;10.244.0.61:15090\u0026quot;, \u0026quot;job\u0026quot;: \u0026quot;envoy-stats\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;istio-system\u0026quot;, \u0026quot;pod_name\u0026quot;: \u0026quot;istio-ingressgateway-6489d9556d-ws6cg\u0026quot;, \u0026quot;reporter\u0026quot;: \u0026quot;source\u0026quot;, \u0026quot;request_protocol\u0026quot;: \u0026quot;http\u0026quot;, \u0026quot;response_code\u0026quot;: \u0026quot;200\u0026quot;, \u0026quot;response_flags\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;source_app\u0026quot;: \u0026quot;istio-ingressgateway\u0026quot;, \u0026quot;source_canonical_revision\u0026quot;: \u0026quot;1.5\u0026quot;, \u0026quot;source_canonical_service\u0026quot;: \u0026quot;istio-ingressgateway\u0026quot;, \u0026quot;source_principal\u0026quot;: \u0026quot;spiffe://cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account\u0026quot;, \u0026quot;source_version\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;source_workload\u0026quot;: \u0026quot;istio-ingressgateway\u0026quot;, \u0026quot;source_workload_namespace\u0026quot;: \u0026quot;istio-system\u0026quot; }, \u0026quot;value\u0026quot;: [ 1588918429.726, \u0026quot;5148\u0026quot; ]  参考 https://stedolan.github.io/jq/manual/#example6 （jq官方手册）\nhttps://jqplay.org （jq playground）\n","id":14,"section":"posts","summary":"Prom返回的json格式没有reindent，看起来很费眼睛。 {\u0026quot;metric\u0026quot;:{\u0026quot;__name__\u0026quot;:\u0026quot;istio_requests_total\u0026quot;,\u0026quot;destination_app\u0026quot;:\u0026quot;productpage\u0026quot;,\u0026quot;destination_service\u0026quot;:\u0026quot;productpage\u0026quot;,\u0026quot;value\u0026quot;:[1588929523.247,\u0026quot;10523\u0026quot;]}]}} 在命令行，想让其用缩进好的json展示出来，就要用到jq。 curl http://test.json | jq 只需要在","tags":["server","jq"],"title":"jq in terminal for json","uri":"https://zecoo.github.io/hugo/2020/05/jq-in-terminal-for-json/","year":"2020"},{"content":" Prom官网给出http API例如获取2015年7月1号某天的数据这样写：（我获得了istio_requests_total的所有metric）\ncurl 'http://localhost:9090/api/v1/query?query=istio_requests_total\u0026amp;time=2015-07-01T20:10:51.781Z'  我想用PromQL，类似Prom的UI针对repose_code对query进行过滤\nquery?istio_requsets_total{response_code=200}  按照普通的思路我试着这样修改curl地址如下：\ncurl 'http://localhost:9090/api/v1/query?query=istio_requsets_total{response_code=200}\u0026amp;time=2020-05-07T20:10:51.781Z'  失败。花了很长时间找资料，找不到。\n最后用浏览器试一下，在http://localhost:9090/api/v1/query?query=之后把PromQL直接粘贴在后面就可以访问到。浏览器这边帮忙做了处理，最后的地址形式是这样的；\nhttp://121.37.159.247:30040/api/v1/query?query=istio_requests_total{response_code=%22200%22}  把引号\u0026quot;处理成了%22。\n参考 https://prometheus.io/docs/prometheus/latest/querying/api/ （官方http API用法）\n","id":15,"section":"posts","summary":"Prom官网给出http API例如获取2015年7月1号某天的数据这样写：（我获得了istio_requests_total的所有metri","tags":["prom"],"title":"PromQL query in http API","uri":"https://zecoo.github.io/hugo/2020/05/promql-query-in-http-api/","year":"2020"},{"content":" 期待数据 10min内的请求数（sum）\nistio_requests_total{destination_service_name=\u0026quot;productpage\u0026quot;}  10min内的请求数增长率（rate）\nrate(istio_requests_total{destination_app=\u0026quot;productpage\u0026quot;}[10m])  10min内的响应时间P90（quantile）\nhistogram_quantile(0.90, sum(rate(istio_request_duration_milliseconds_bucket{destination_app=\u0026quot;productpage\u0026quot;}[10m])) by(le))  10min内的响应时间增长率（rate）\nrate(istio_request_duration_milliseconds_bucket{destination_app=\u0026quot;productpage\u0026quot;}[10m])  参考 https://www.zhihu.com/question/380615839 （Prom时区UTC没法改）\n","id":16,"section":"posts","summary":"期待数据 10min内的请求数（sum） istio_requests_total{destination_service_name=\u0026quot;productpage\u0026quot;} 10min内的请求数增长率（rate） rate(istio_requests_total{destination_app=\u0026quot;productpage\u0026quot;}[10m]) 10min内的响应时间P90（quantile） histogram_quantile(0.90, sum(rate(istio_request_duration_milliseconds_bucket{destination_app=\u0026quot;productpage\u0026quot;}[10m])) by(le)) 10m","tags":["prom"],"title":"PromQL理解","uri":"https://zecoo.github.io/hugo/2020/05/promql%E7%90%86%E8%A7%A3/","year":"2020"},{"content":" siege一分钟压测最后给出的统计数据：\nLifting the server siege... Transactions:\t439 hits Availability:\t51.83 % Elapsed time:\t59.33 secs Data transferred:\t8.47 MB Response time:\t9.88 secs Transaction rate:\t7.40 trans/sec Throughput:\t0.14 MB/sec Concurrency:\t73.10 Successful transactions: 439 Failed transactions:\t408 Longest transaction:\t56.69 Shortest transaction:\t0.13  istio Envoy的access log：\n【START_TIME】[2020-05-06T09:32:24.488Z] 【METHOD】\u0026quot;GET 【PATH】/flasgger_static/swagger-ui-bundle.js 【PROTOCOL】HTTP/1.1\u0026quot; 【REPONSE CODE】200 - \u0026quot;-\u0026quot; \u0026quot;-\u0026quot; 0 1048149 261 225 【FORWARD_FOR】\u0026quot;-\u0026quot; 【AGENT】\u0026quot;Mozilla/5.0 (pc-x86_64-linux-gnu) Siege/4.0.5\u0026quot; 【REQUEST_ID】\u0026quot;51b45953-9ede-9390-b8b5-162247165a5b\u0026quot; 【AUTHORITY】\u0026quot;10.108.208.232:8000\u0026quot; 【HOST】\u0026quot;127.0.0.1:80\u0026quot; 【SERVICE】inbound|8000|http|httpbin.default.svc.cluster.local 【CLUSTER_IP】127.0.0.1:43000 10.244.0.67:80 10.244.0.1:10504 - 【APP】default  Prom关于duration的metric：\n\u0026quot;metric\u0026quot;:{ \u0026quot;__name__\u0026quot;:\u0026quot;istio_request_duration_milliseconds_bucket\u0026quot;, \u0026quot;connection_security_policy\u0026quot;:\u0026quot;unknown\u0026quot;, \u0026quot;destination_app\u0026quot;:\u0026quot;productpage\u0026quot;, \u0026quot;destination_canonical_revision\u0026quot;:\u0026quot;v1\u0026quot;, \u0026quot;destination_canonical_service\u0026quot;:\u0026quot;productpage\u0026quot;, \u0026quot;destination_service\u0026quot;:\u0026quot;productpage.default.svc.cluster.local\u0026quot;, \u0026quot;destination_service_name\u0026quot;:\u0026quot;productpage\u0026quot;, \u0026quot;destination_service_namespace\u0026quot;:\u0026quot;default\u0026quot;, \u0026quot;destination_version\u0026quot;:\u0026quot;v1\u0026quot;, \u0026quot;destination_workload\u0026quot;:\u0026quot;productpage-v1\u0026quot;, \u0026quot;destination_workload_namespace\u0026quot;:\u0026quot;default\u0026quot;, \u0026quot;instance\u0026quot;:\u0026quot;10.244.0.14:15090\u0026quot;, \u0026quot;job\u0026quot;:\u0026quot;envoy-stats\u0026quot;, \u0026quot;le\u0026quot;:\u0026quot;6000\u0026quot;, \u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;, \u0026quot;pod_name\u0026quot;:\u0026quot;ratings-v1-6f855c5fff-wsqpx\u0026quot;, \u0026quot;reporter\u0026quot;:\u0026quot;source\u0026quot;, \u0026quot;request_protocol\u0026quot;:\u0026quot;http\u0026quot;, \u0026quot;response_code\u0026quot;:\u0026quot;200\u0026quot;, \u0026quot;response_flags\u0026quot;:\u0026quot;-\u0026quot;,a \u0026quot;source_app\u0026quot;:\u0026quot;ratings\u0026quot;, \u0026quot;source_canonical_revision\u0026quot;:\u0026quot;v1\u0026quot;, \u0026quot;source_canonical_service\u0026quot;:\u0026quot;ratings\u0026quot;, \u0026quot;source_principal\u0026quot;:\u0026quot;spiffe://cluster.local/ns/default/sa/bookinfo-ratings\u0026quot;, \u0026quot;source_version\u0026quot;:\u0026quot;v1\u0026quot;, \u0026quot;source_workload\u0026quot;:\u0026quot;ratings-v1\u0026quot;, \u0026quot;source_workload_namespace\u0026quot;:\u0026quot;default\u0026quot;}, \u0026quot;values\u0026quot;:[[1588835634.769,\u0026quot;2\u0026quot;],[1588835649.769,\u0026quot;2\u0026quot;],[1588835664.769,\u0026quot;2\u0026quot;],[1588835679.769,\u0026quot;2\u0026quot;]] },  参考 https://prometheus.io/docs/prometheus/latest/querying/api/ （curl query获取metric）\n","id":17,"section":"posts","summary":"siege一分钟压测最后给出的统计数据： Lifting the server siege... Transactions: 439 hits Availability: 51.83 % Elapsed time: 59.33 secs Data transferred: 8.47 MB Response time: 9.88 secs Transaction rate: 7.40 trans/sec Throughput: 0.14 MB/sec Concurrency: 73.10 Successful transactions: 439 Failed transactions: 408 Longest transaction: 56.69 Shortest transaction: 0.13 istio Envoy的","tags":["istio","prom"],"title":"Meta Metrics","uri":"https://zecoo.github.io/hugo/2020/05/meta-metrics/","year":"2020"},{"content":" 基本数据这样\n【START_TIME】[2020-05-06T09:32:24.488Z] 【METHOD】\u0026quot;GET 【PATH】/flasgger_static/swagger-ui-bundle.js 【PROTOCOL】HTTP/1.1\u0026quot; 【REPONSE CODE】200 - \u0026quot;-\u0026quot; \u0026quot;-\u0026quot; 0 1048149 261 225 【FORWARD_FOR】\u0026quot;-\u0026quot; 【AGENT】\u0026quot;Mozilla/5.0 (pc-x86_64-linux-gnu) Siege/4.0.5\u0026quot; 【REQUEST_ID】\u0026quot;51b45953-9ede-9390-b8b5-162247165a5b\u0026quot; 【AUTHORITY】\u0026quot;10.108.208.232:8000\u0026quot; 【HOST】\u0026quot;127.0.0.1:80\u0026quot; inbound|8000|http|httpbin.default.svc.cluster.local 127.0.0.1:43000 10.244.0.67:80 10.244.0.1:10504 - default  0 135 1 1 (/status/418) 0 9593 73 73 (http/1.1) 0 1428809 1 1 (swagger-ui.bundle.js) 0 85578 1 0 (jquery.min.js) 0 0 118 - (DC ResponseCode:0) 0 1048149 261 225 (swagger-ui.bundle.js)  那么这4串数字分别代表什么呢？根据Envoy官方给出的对应关系，由于HTTP_FLAGS一般都是比较特别的，而我得到的都是\u0026rdquo;-\u0026ldquo;，可以得到对应关系：\n[1]BYTES_RECEIVED [2]BYTES_SENT [3]DURATION [4]ENVOY-UPSTREAM-SERVICE-TIME  istio bookinfo示例中给出的log是这样（官方给出的，我现在不想折腾Mixer，得不到这个数据）\n{\u0026quot;level\u0026quot;:\u0026quot;warn\u0026quot;,\u0026quot;time\u0026quot;:\u0026quot;2018-09-15T20:46:35.982761Z\u0026quot;,\u0026quot;instance\u0026quot;:\u0026quot;newlog.xxxxx.istio-system\u0026quot;,\u0026quot;destination\u0026quot;:\u0026quot;productpage\u0026quot;,\u0026quot;latency\u0026quot;:\u0026quot;968.030256ms\u0026quot;,\u0026quot;responseCode\u0026quot;:200,\u0026quot;responseSize\u0026quot;:4415,\u0026quot;source\u0026quot;:\u0026quot;istio-ingressgateway\u0026quot;,\u0026quot;user\u0026quot;:\u0026quot;unknown\u0026quot;}  区别的重点在于 samples/bookinfo/telemetry/log-entry.yaml 这个文件。\n但是如果要log的话，要用到Mixer这个组件，然而Mixer已经被官方弃用了，怎么官方文档还是要这玩意儿？\n中文文档更新得好慢啊，Envoy获取log已经不在英文的文档里了…\n参考 https://github.com/istio/istio/tree/master/mixer （Mixer被弃用）\nhttps://istio-releases.github.io/v0.1/docs/tasks/installing-istio.html （安装Mixer）\nhttps://www.envoyproxy.cn/configurationreference/accesslogging （envoy access log format 中文文档）\nhttps://blog.csdn.net/luo15242208310/article/details/98745143 （用彩色区分了不同的log部分）\n","id":18,"section":"posts","summary":"基本数据这样 【START_TIME】[2020-05-06T09:32:24.488Z] 【METHOD】\u0026quot;GET 【PATH】/fl","tags":["istio"],"title":"istio envoy log type","uri":"https://zecoo.github.io/hugo/2020/05/istio-envoy-log-type/","year":"2020"},{"content":" SCSS：Sassy CSS\nSASS比SCSS更简洁啊，感觉更像现代编程语言，跟yaml一样。\n我咋还在scss的环境上踩到坑了呢？\n先是给我报错\nbad interpreter: /System/Library/Frameworks/Ruby.framework/Versions/2.3/usr/bin/ruby: no such file or directory  一查是sass环境没装上……我以为mac自带ruby不用管呢\nsudo gem install -n /usr/local/bin sass --pre  安装之。然后发现ruby的源用不了，原来是 https://gems.ruby-china.org 改成了 https://gems.ruby-china.com ，害。\n编译css的项目目录也需要更改，默认是编译到scss相同的目录。\n{ \u0026quot;cmd\u0026quot;: [ \u0026quot;sass\u0026quot;, \u0026quot;--update\u0026quot;, \u0026quot;$file:${file_path}/../css/${file_base_name}.css\u0026quot;, \u0026quot;--stop-on-error\u0026quot;, \u0026quot;--no-cache\u0026quot;], \u0026quot;osx\u0026quot;: { \u0026quot;path\u0026quot;: \u0026quot;/usr/local/bin:$PATH\u0026quot; } }  最后sublime再装一个插件\u0026rdquo;sublimeBuildOnSave\u0026rdquo;自动编译sass，不要每次都cmd+B一下。\n参考 https://www.jianshu.com/p/f8cbe91498dc （ruby china换源了）\nhttps://www.sass.hk/install/ （ruby 安装sass）\nhttps://blog.logrocket.com/getting-started-with-bootstrapvue-2d8bf907ef11/ （bootVue blog）\nhttps://bootstrap-vue.org/docs （bootVue官网）\n","id":19,"section":"posts","summary":"SCSS：Sassy CSS SASS比SCSS更简洁啊，感觉更像现代编程语言，跟yaml一样。 我咋还在scss的环境上踩到坑了呢？ 先是给我报错 bad interpreter:","tags":["css","scss"],"title":"scss tutorial","uri":"https://zecoo.github.io/hugo/2020/05/scss-tutorial/","year":"2020"},{"content":" 先提一个题外话：如何销毁已经创建的部署？答：\n有kubectl create -f ./metrics-server就有kubectl delete -f ./metrics-server\n按照主要参考，大部分都可以实现。但是有几个坑需要注意\n autoscaling的版本分为v1beta1、v2beta1、v2beta2。用法不同 deployment: extension/v1beta1这样的用法我改为apps/v1beta1创建HPA才能成功  HPA的重点是hpa.yaml里面的cpu和mem的指定。\nPrometheus + HPA的重点是如何将Prom的数据改为metrics server可以使用的数据。这里的方法应该是config-map中构建的一系列rule。先拿来用，暂时不去深究。\n参考 https://blog.csdn.net/yevvzi/article/details/79561150 （本文主要参考）\nhttps://blog.csdn.net/xktxoo/article/details/87909432 （jq介绍）\nhttps://cloud.tencent.com/developer/article/1394657 （selector问题）\nhttps://q.cnblogs.com/q/125354/ （target问题）\nhttps://blog.51cto.com/13740724/2368066 （新版本该这么用）\nhttps://www.cnblogs.com/yunqishequ/p/10006896.html （不同版本autoscaling）\nhttps://stackoverflow.com/questions/43163625/when-i-use-deployment-in-kubernetes-whats-the-differences-between-apps-v1beta1 （deployment version）\n","id":20,"section":"posts","summary":"先提一个题外话：如何销毁已经创建的部署？答： 有kubectl create -f ./metrics-server就有kubectl delete -f ./metrics-server 按照主要参考，大部分","tags":["k8s","HPA"],"title":"k8s HPA尝试","uri":"https://zecoo.github.io/hugo/2020/05/k8s-hpa%E5%B0%9D%E8%AF%95/","year":"2020"},{"content":" Prom的四种基本metric类型  counter：从0开始计数的，比如http_req_total gauge：有浮动的指标，比如cpu、mem histogram：统计数据，比如P90 summary：和histogram类似  几个示例 P90  The φ-quantile is the observation value that ranks at number φ*N among the N observations. Examples for φ-quantiles: The 0.5-quantile is known as the median. The 0.95-quantile is the 95th percentile.\n 分位点的概念。官方示例：10分钟内请求持续时间的90%，以PromQL的形式给出：\nhistogram_quantile(0.9, rate(http_request_duration_seconds_bucket[10m]))  以下数据是通过 curl http://121.37.159.247:30040/metrics 获取的，截取部分。\n# HELP prometheus_tsdb_wal_fsync_duration_seconds Duration of WAL fsync. # TYPE prometheus_tsdb_wal_fsync_duration_seconds summary prometheus_tsdb_wal_fsync_duration_seconds{quantile=\u0026quot;0.5\u0026quot;} 0.012352463 prometheus_tsdb_wal_fsync_duration_seconds{quantile=\u0026quot;0.9\u0026quot;} 0.014458005 prometheus_tsdb_wal_fsync_duration_seconds{quantile=\u0026quot;0.99\u0026quot;} 0.017316173 prometheus_tsdb_wal_fsync_duration_seconds_sum 2.888716127000002 prometheus_tsdb_wal_fsync_duration_seconds_count 216  “从上面的样本中可以得知当前Prometheus Server进行wal_fsync操作的总次数为216次，耗时2.888716127000002s。其中中位数（quantile=0.5）的耗时为0.012352463，9分位数（quantile=0.9）的耗时为0.014458005s，90%的数据都小于等于0.014458005s。”\nle=\u0026ldquo;0.3\u0026rdquo; le即level，我现在只有在官网看到对le的简单解释，而且理解起来就是duration\u0026lt;level。\n5分钟内响应时间在300ms以下的请求（le=\u0026ldquo;0.3\u0026rdquo; means requests within 300ms）\nsum(rate(http_request_duration_seconds_bucket{le=\u0026quot;0.3\u0026quot;}[5m])) by (job)  请详细介绍request_duration_seconds_bucket所代表的含义：\nrequest durations in seconds（请求响应时间用单位秒表示），bucket表示所有采样点的数量。结合le=\u0026quot;0.3\u0026quot;表示\u0026rdquo;响应时间小于300ms的采样点的数量。\n更直观理解桶？可以看参考中的博客。\nrate 增长率。1分钟内的cpu使用率：\nrate(process_cpu_seconds_total[1m])* 100  key/value的metrics形式 例如：\nprocess_max_fds 65535  表示当前采集的最大文件句柄数是65535。\n埋点Prom 看着真的很简单啊，参考python的埋点方法：\nfrom prometheus_client import start_http_server, Summary import random import time # Create a metric to track time spent and requests made. REQUEST_TIME = Summary('request_processing_seconds', 'Time spent processing request') # Decorate function with metric. @REQUEST_TIME.time() def process_request(t): \u0026quot;\u0026quot;\u0026quot;A dummy function that takes some time.\u0026quot;\u0026quot;\u0026quot; time.sleep(t) if __name__ == '__main__': # Start up the server to expose the metrics. start_http_server(8000) # Generate some requests. while True: process_request(random.random())  然后就可以在localhost:8000看到prom metrics了。\n自定义的metric，别人能做成这样：\n最后的灵魂一问：如何知道Prom有哪些可用的metrics呢？\n参考 https://www.v2ex.com/t/606786 （中文讲Prom metrics最详细的文章了吧）\nhttps://www.jianshu.com/p/15f929160f38 （四种metric类型介绍得很棒）\nhttps://prometheus.io/docs/concepts/metric_types/ （官方解释metrics type）\nhttps://prometheus.io/docs/prometheus/latest/querying/functions/#histogram_quantile （P90官方说明）\nhttps://github.com/prometheus/client_python#histogram （python埋点方法）\nhttps://www.cnblogs.com/YaoDD/p/11391316.html （go埋点获取metrics详解）\nhttps://prometheus.io/docs/practices/histograms/ （histogram官方介绍）\nhttps://www.cnblogs.com/arloblog/p/12162858.html （这才是介绍Prom metric最详细的文章，其实是官网翻译哈哈，但是翻译的好～）\nhttp://blog.itpub.net/28218939/viewspace-2658770/ （埋点自定义metric并grafana化）\nhttps://www.jianshu.com/p/8a6d3eff31e7 （直观理解bucket 唯一一个讲解清楚的博客）\n","id":21,"section":"posts","summary":"Prom的四种基本metric类型 counter：从0开始计数的，比如http_req_total gauge：有浮动的指标，比如cpu、me","tags":["prom"],"title":"Prometheus Metric形式","uri":"https://zecoo.github.io/hugo/2020/05/prom-metric%E5%BD%A2%E5%BC%8F/","year":"2020"},{"content":" 环境都搭好了，不知道里面的原理那可不行。\n我也不会系统写，想到哪里就写到哪里吧。\nistio是如何做到无侵入就能控制流量转发？ Sidecar还有Envoy是这里的核心。\n这个图就很好得说明了sidecar中流量的走向。这部分在华为的书《云原声服务网格istio》中关于sidecar的介绍写得很清楚。也就是说Envoy通过iptables拦截了进来的流量，然后强迫流量走自己的通道，相当于一个收保护费的。\n那么iptables为什么这么屌，能把流量给拦截下来？其实iptables改名叫netfilter更形象一些。先不深究iptables是如何做转发的，形象理解iptables为何这么屌，其实是它作为一个内核设置的功能，可以把网卡接受到的流量先通过自己过滤，然后再发送给web应用。\n那么也就是，我从Safari发起对http://serverip:port/productpage访问，首先流量通过我的网卡，经过计算机网络传到server的网卡，然后server的网卡把这条流量先交给iptables过滤一下，然后再发给productpage代表的微服务。\niptables我好像把它关了，但是我的istio依然能够工作？这又是为什么呢…\nk8s如何调度微服务节点的 搞清楚这样几个概念：pod、deployment、service、node\n 一个pod上跑k个容器，这k个容器组成一个app（微服务） deployment，其实叫replica controller更合适。顾名思义，就是扩缩pod service就是app对外的一个访问入口。一个svc中可能有n个replica node就是部署service的节点  那么到这里我有一个小问题，有如果有n个replica，那么流量进来了会被分配到哪个pod里呢？\nkubelet是什么 kubelet是node的proxy。\nk8s的DNS是什么 给每个svc可用地址。\n（以上两个百度结果都不太好，姑且这么理解吧）\n参考 http://www.zsythink.net/archives/1199/ （讲iptables的好文）\n","id":22,"section":"posts","summary":"环境都搭好了，不知道里面的原理那可不行。 我也不会系统写，想到哪里就写到哪里吧。 istio是如何做到无侵入就能控制流量转发？ Sidecar还有","tags":["istio"],"title":"K8s + Istio 概念","uri":"https://zecoo.github.io/hugo/2020/04/k8s-istio-%E6%A6%82%E5%BF%B5/","year":"2020"},{"content":" Prometheus 基本的metrics监测插件。\n通过query查询不同的信息，例如以下信息就是istio_requests_total这条query查询到的n条数据中的一条。（不全，删除了部分我不关注的信息）\nistio_requests_total{destination_app=\u0026quot;productpage\u0026quot;,destination_principal=\u0026quot;spiffe://cluster.local/ns/default/sa/bookinfo-productpage\u0026quot;,destination_service=\u0026quot;productpage.default.svc.cluster.local\u0026quot;,destination_service_name=\u0026quot;productpage\u0026quot;,destination_service_namespace=\u0026quot;default\u0026quot;,destination_version=\u0026quot;v1\u0026quot;,destination_workload=\u0026quot;productpage-v1\u0026quot;,instance=\u0026quot;10.244.0.24:15090\u0026quot;,job=\u0026quot;envoy-stats\u0026quot;,namespace=\u0026quot;istio-system\u0026quot;,pod_name=\u0026quot;istio-ingressgateway-6489d9556d-bc48z\u0026quot;,response_code=\u0026quot;503\u0026quot;,source_workload=\u0026quot;istio-ingressgateway\u0026quot;,source_workload_namespace=\u0026quot;istio-system\u0026quot;}  Grafana Metrics可视化插件。\nRequest Volume代表什么？\nRequest Duration中的P50、P90分别代表什么？\nJaegar 主要显示调用了哪些微服务，调用顺序是什么样的，响应时间是多少。\nKiali 链路追踪可视化插件。可以看出有几个微服务，调用关系是什么样的。\n里面也有P50、P90。说明这个很关键啊。\nP90=100ms，就是说90%的请求其响应时间在100ms以内，剩余10%的响应时间大于100ms。\nSiege 压测工具。来看看压测结果\n$ siege -d 10 -c 200 -t 2 http://121.37.159.247:32753/productpage Lifting the server siege... Transactions:\t519 hits Availability:\t99.24 % Elapsed time:\t119.34 secs Data transferred:\t20.14 MB Response time:\t9.01 secs Transaction rate:\t4.35 trans/sec Throughput:\t0.17 MB/sec Concurrency:\t39.17 Successful transactions: 519 Failed transactions:\t4 Longest transaction:\t110.01 Shortest transaction:\t0.08  模拟200个用户，每间隔10ms发送一次请求，持续2分钟。\n参考 https://blog.dianduidian.com/post/percentile-百分位数/（p50、p90讲解）\n","id":23,"section":"posts","summary":"Prometheus 基本的metrics监测插件。 通过query查询不同的信息，例如以下信息就是istio_requests_total这条query查询到的","tags":["istio","prom"],"title":"Istio 可视化插件概览","uri":"https://zecoo.github.io/hugo/2020/04/istio-%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8F%92%E4%BB%B6%E6%A6%82%E8%A7%88/","year":"2020"},{"content":" 如果是Microk8s安装，很简单就一步：\nmicrok8s.enable istio  好像pull istio镜像的过程特别漫长。由于用的是力的2g服务器，到这里内存爆炸，用不了了。\n然后转战华为云15天4g服务器试用。\n用Kubeadm安装k8s，过程呢，在另一篇博客中。\n这里主要回顾Kubeadm安装istio以及各种可视化插件的过程。\n全程请把精力集中在istio的官网上。每个教程都很详细。\n下载tar包，然后解压得到istio-15.2目录。这个目录里包含需要部署的yaml文件以及bookinfo的实例。\n然后按照istio官方的教程去安装就OK了。\n重点呢，是各种可视化插件的部署。如果istio部署顺利的话，各种插件的svc已经启动了，可以get svc查看一下。但是到目前为止还是只能在集群内访问。访问的入口是istio-ingressgateway。然后按照istio官方给出的远程访问方式去部署就好了。这里唯一一个和官方给出的教程不同的地方是，以Prometheus为例，官方给出的访问地址是：\n Prometheus: http://\u0026lt;IP ADDRESS OF CLUSTER INGRESS\u0026gt;:15030/  这里的ingressip要改为\n[服务器地址] + [svc istioingressgateway对于15030暴露的端口]\n然后就可以用上面的地址访问到Prometheus了。\n当然我还搜到了另外一种方式访问，把Prometheus用另一个svc包裹起来，相当于多了一个nodeport svc的中介。输入这个命令即可：\nkubectl expose service prometheus --type=NodePort \\ --name=prometheus-svc --namespace istio-system  然后就会多出来一个名为prometheus-svc的nodeport形式的服务。调用这个服务暴露出来的地址就可以访问Prometheus，只是我觉得这个方法可能不太安全，就没有用。\n参考 https://www.jianshu.com/p/b72c1e06b140 （安装指南）\nhttps://www.cnblogs.com/assion/p/11326088.html （修改istiogateway的LB为nodeport）\nhttps://www.jianshu.com/p/b72c1e06b140 （用hello-node做示例）\nhttps://www.cnblogs.com/davidwang456/articles/9311470.html （SLA和SLO的关系）\nhttps://www.jianshu.com/p/fd90d4914505 （istio的良好实践）\nhttps://www.jianshu.com/p/bed143a1c886 （估计也是个研究生大神，给王老师演示这个就可以了）\nhttps://www.cnblogs.com/CCE-SWR/p/10286404.html （也是演示之用）\nhttps://www.ibm.com/support/knowledgecenter/en/SSBS6K_2.1.0.3/manage_cluster/istio.html （另一种暴露Prometheus的方式）\n","id":24,"section":"posts","summary":"如果是Microk8s安装，很简单就一步： microk8s.enable istio 好像pull istio镜像的过程特别漫长。由于用的是力的2g服务器，到这里内存爆炸，用不了了。","tags":["istio","hello-world"],"title":"Istio 安装回顾","uri":"https://zecoo.github.io/hugo/2020/04/istio-%E5%AE%89%E8%A3%85%E5%9B%9E%E9%A1%BE/","year":"2020"},{"content":" 装好的那一瞬间，我感动得要哭了。我现在也算半个k8s运维小能手了吧。\n安装K8s有这几种方法\n minikube mircok8s kubeadm  这三种方法我都用过。minikube在win上以虚拟机的形式运行，挺麻烦的讲道理。microk8s是最友好的方式，解决一个拉镜像的问题，也就不成问题了。kubeadm应该是最麻烦的了。\n回归正题，记录整个安装以及填坑的过程。\n首先docker要安装上。\n然后，kubelet、kubeadm、kubectl、kubernetes-cni一套安装下来。\n我现在才看到参考1中的一句话：\n# 指定版本否则都会默认安装库中最新版本，会因为彼此依赖的版本不同安装失败 $ yum install -y kubelet-1.13.1 kubeadm-1.13.1 kubectl-1.13.1 kubernetes-cni-0.6.0 # 设置开机启动并启动kubelet $ systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet  看到第一句话了吗？给我大声读几遍！！我就是因为这个，吃了多少亏，搜索多少资料😭。请一定保证你的kubelet、kubeadm、kubectl的版本相同。\n第二步，kubeadm config images list列出所有需要的image，因为国内网络问题嘛，一样的。然后用以下bash脚本安装好，docke images检查一下。\nfor i in `kubeadm config images list`; do imageName=${i#k8s.gcr.io/} docker pull registry.aliyuncs.com/google_containers/$imageName docker tag registry.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName docker rmi registry.aliyuncs.com/google_containers/$imageName done;  第三步，kubeadm init --pod-network-cidr=10.244.0.0/16初始化kubeadm。这一步最重要的是/etc/kubernetes/admin.conf这个文件。还有后面那个参数，如果不加上，就会遇到新的坑哦～\ninit结束之后，不要忘了提示的三行命令\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  第四步，到了这里，我们发现所有的pod都运行挺好的，除了coredns。但是镜像也都下载好了，为什么宁就比较特殊呢？然后我describe了一下，发现和flannel有关。\n所以啦，就是要先把flannel给部署好。注意版本问题。\n 版本1.13.1使用的是github上flannel-old.yml文件。而1.17以上版本用的是flannel.yml文件。\n 这里还有一个坑，flannel是quay.io库的镜像，国内也是访问不到的，describe flannel pod就能看到该下载哪个版本的flannel镜像。还有刚才提到的init后面的参数，如果没有添加这个参数的话，flannel部署也会出问题的😊。好的，flannel解决掉，coredns也就马上部署好了。\n最后一套检查一下\nkubectl get node kubectl get sc kubectl get pod --all-namespaces  参考 https://blog.csdn.net/zjcjava/article/details/99317569 （我按照这篇来装的！）\nhttps://blog.csdn.net/woay2008/article/details/93250137 （三行命令没敲能把人愁死）\nhttps://www.cnblogs.com/gscienty/p/10586118.html （简洁版安装过程）\nhttps://www.kubernetes.org.cn/5462.html （带dashboard的教程）\nhttps://www.cnblogs.com/caibao666/p/11664726.html （flannel的那个坑）\nhttps://blog.csdn.net/u012547633/article/details/103846564 （另一个也喜欢记录的小伙伴）\nhttps://www.cnblogs.com/qq952693358/p/6537846.html （E: Unable to lock directory /var/lib/apt/lists/ 错误）\nhttps://learnku.com/articles/29209 （k8s-pull.sh）\n","id":25,"section":"posts","summary":"装好的那一瞬间，我感动得要哭了。我现在也算半个k8s运维小能手了吧。 安装K8s有这几种方法 minikube mircok8s kubeadm 这三种方法我都用过。minikube在win","tags":["k8s","kubeadm"],"title":"Kubeadm 安装记录","uri":"https://zecoo.github.io/hugo/2020/04/kubeadm-%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95/","year":"2020"},{"content":" 每天捣鼓k8s就是各种填坑。不过在查问题解决办法的时候有个人说\n 遇到坑不要怕，百度谷歌就是了。填完了坑，再踩两脚，以后走起来就平了。\n 说得真好。爆炸的心态慢慢也就平复了。\n回归正题，坑长这个样子：\nkubectl -f apply any.yaml  都会报错：\nError from server (NotFound): the server could not find the requested resource  一开始我以为是apiserver的问题，但是apiserver是正常运行的。搜索了很长时间，还是在yandex上搜索到可能是Kubectl cli和server的版本差距过大造成的问题。\n看一下我的两个版本：kubectl version\nClient Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;9\u0026quot;, GitVersion:\u0026quot;v1.9.3\u0026quot;, GitCommit:\u0026quot;5fa2db2bd46ac79e5e00a4e6ed24191080aa463b\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-01-18T21:12:46Z\u0026quot;, GoVersion:\u0026quot;go1.9.2\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;darwin/amd64\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;16\u0026quot;, GitVersion:\u0026quot;v1.17.5\u0026quot;, GitCommit:\u0026quot;72c30166b2105cd7d3350f2c28a219e6abcd79eb\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2020-01-18T23:23:21Z\u0026quot;, GoVersion:\u0026quot;go1.13.5\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  可以看到cli的版本是v1.9.3，而server的版本是1.17.5。差的太多了。\n继续搜索如何升级kubectl cli之，百度就别想能找到答案了。在k8s官网找到了安装kubectl的方法，重新安装了一下，kubectl cli的版本就升级到最新v1.18.5了。\n这下再试一下\nkubectl apply -f anyfile.yaml  终于可以运行yaml文件了。\n参考 https://kubernetes.io/docs/tasks/tools/install-kubectl/#before-you-begin （升级kubectl cli）\nhttps://devops.stackexchange.com/questions/2956/how-do-i-get-kubernetes-to-work-when-i-get-an-error-the-server-could-not-find-t （坑出现的原因1）\nhttps://www.digitalocean.com/community/questions/kubectl-apply-f-deployment-yml-the-server-could-not-find-the-requested-resource （坑出现的原因2）\n","id":26,"section":"posts","summary":"每天捣鼓k8s就是各种填坑。不过在查问题解决办法的时候有个人说 遇到坑不要怕，百度谷歌就是了。填完了坑，再踩两脚，以后走起来就平了。 说得真好。","tags":["k8s","kubectl"],"title":"Kubectl client 和 server version 差距错误","uri":"https://zecoo.github.io/hugo/2020/04/kubectl-client-%E5%92%8C-server-version-%E5%B7%AE%E8%B7%9Dbug/","year":"2020"},{"content":" 做这个任务的时候我一度想要放弃。因为基础不牢，觉得这东西对我来说有点难啊。结果磕磕绊绊借鉴DIYgod的网页做出来之后，发现这东西真tm是基础。\n我也百度了一下，发现写task0002_5的博客少之又少，说明ife的任务2到这里，基本上已经没有很多人在做了。\n瞎总结 首先要了解事件是个什么东西。在这个任务里面，要监听的是\n 滑块被拖动时的位置 dragStart(e) 滑块处于拖动状态 dragging(e) 滑块最后放置的位置 drop(e)  而在js里，有专门针对拖动的监听事件*dragStart*和*drop*方法。\n让滑块动起来 首先要给滑块设置draggable=true，然后就会神奇得发现滑块真的可以拖动了。留下一个残影在原来的位置。但是也会神奇得发现鼠标一松开还是要打回原形。\n先做第一个动画，拖动一个滑块A的时候，首先A要在wrap中消失。可以用css添加一个class dragging，设置display=none。监听滑块处在拖动的状态时，就是不可见的。而一旦落了脚，要移除这个class。重新回到可见状态。\n第一个动画做完，可以发现，拖动A，A下面的滑块会自动补上A的位置。松开鼠标依旧打回原形。\n单个容器拖动效果 然后尝试做单个容器内滑块的拖动效果。第二个动画该考虑把要移动的滑块A插到合适的地方。这里面有这样几步\n 获取当前滑块A的中心相对容器的坐标。得出插入第k格 将A插入指定位置 A下面的滑块均向下移动一格  我在这里偷了个懒，仅根据drop事件当前的鼠标位置来判断插入哪一格。具体细节先不追究，毕竟还要抓紧学vue的知识。2、3步也比较简单，让每个滑块position=absolute，然后设置一个top值，就可以控制滑块的移动了。\n多容器拖动效果 最后将单容器拓展到双容器甚至多容器，其实仅比单容器多了一步，就是根据滑块drop的中心位置判断落在哪个容器。\n踩坑记 我写回顾主要是想记一下自己踩的坑。\n$.on(document.body, 'dragover', dragOver);  没错，全在这一行代码里了。\n$.on是util.js里addEvent(element, event, listener)的封装。\n element，就是监听对象。我在做drop效果的时候，打死都出不来。随便一拖动，滑块就没了。最后定位到是element的问题，我打开html检查器，发现自己没有定义container的高度，结果就是整个body只有一丢丢高，并没有把容器包括进去。所以我拖动的滑块，其实在监听对象body之外。那肯定是没得效果咯。 event，这…我本来以为没啥实际意义的参数，又给我栽一大跟头，让我不好好看文档先学习用法。event要和listener方法名一致，并且全部小写。 dragOver别看就一行代码，e.preventDefault();，这是为了防止浏览器拖动结束没来得及drop就结束监听。不然又会遇到一拖动滑块就消失了的情况。  以上。\n参考 https://www.runoob.com/jsref/event-ondrag.html （菜鸟onDrag事件）\nhttps://www.jianshu.com/p/2dfb870e0b88 （本任务类型）\n","id":27,"section":"posts","summary":"做这个任务的时候我一度想要放弃。因为基础不牢，觉得这东西对我来说有点难啊。结果磕磕绊绊借鉴DIYgod的网页做出来之后，发现这东西真tm是基","tags":["js","ife"],"title":"IFE task0002_5 回顾","uri":"https://zecoo.github.io/hugo/2020/04/ife-task0002_5-%E5%9B%9E%E9%A1%BE/","year":"2020"},{"content":" 目标 最小(大)化一个没有具体表达式的函数。\n举个例子 给定一个函数 $$ f(x,y) = -x^2 - (y-1)^2 +1 $$\nBO通过几次迭代找到能让 f(x,y) 最小的x和y的值。\n吐槽一下 中文的科普环境能不能再差一点？都尼玛是秀智商的。\n一个简单的东西讲得巨鸡儿复杂，还觉得自己抖机灵挺可爱的。说得就是这篇博客。\n幸好让我遇到了还算可以的另一篇博客。\n当然都不及老外的blog。\n我真的不是崇洋媚外。就是在讲一个事实。科学的范畴，中文优秀博客少得可怜。至少百度出来的是这样。\n","id":28,"section":"posts","summary":"目标 最小(大)化一个没有具体表达式的函数。 举个例子 给定一个函数 $$ f(x,y) = -x^2 - (y-1)^2 +1 $$ BO通过几次迭代找到能让 f(x,y) 最小的x和y的值。 吐槽一下 中文的科普","tags":["math"],"title":"Bayesian Optimization 直观理解","uri":"https://zecoo.github.io/hugo/2020/04/bayesian-optimization/","year":"2020"},{"content":" 上一节讲到现在可以在vps上访问到hello-world了。\n然而我想把访问操作放在我的电脑上。我没有想到NodePort这么容易就能做到了。修改一下service.yaml里面port的type，从ClusterIP改为NodePort就可以了。\n到这里我已经很满足了，但是手痒痒想要看看dashboard的情况。首先嘛，得先把dashboard服务给启动起来对不对？不幸的是，在之前装microk8s的时候，我使用了这样一行命令添加了dns和dashboard这两个add-on\nkubectl enable dns dashboard  结果就是，dashboard其实已经启动了，只是我一直使用的命令\nkubectl get svc  只能显示namespace为default的service，就看不到dashboard也已经启动了。\n如果想要查看所有的svc还是添加--all-namespaces选项。\nubuntu@VM-0-12-ubuntu:~$ sudo kubectl get svc --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE container-registry registry NodePort 10.152.183.11 \u0026lt;none\u0026gt; 5000:32000/TCP 13d default hello-node NodePort 10.152.183.14 \u0026lt;none\u0026gt; 8080:31908/TCP 4d4h default kubernetes ClusterIP 10.152.183.1 \u0026lt;none\u0026gt; 443/TCP 13d kube-system dashboard-metrics-scraper ClusterIP 10.152.183.247 \u0026lt;none\u0026gt; 8000/TCP 13d kube-system heapster ClusterIP 10.152.183.227 \u0026lt;none\u0026gt; 80/TCP 13d kube-system kube-dns ClusterIP 10.152.183.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 13d kube-system kubernetes-dashboard NodePort 10.152.183.31 \u0026lt;none\u0026gt; 443:32570/TCP 13d kube-system monitoring-grafana ClusterIP 10.152.183.21 \u0026lt;none\u0026gt; 80/TCP 13d kube-system monitoring-influxdb ClusterIP 10.152.183.102 \u0026lt;none\u0026gt; 8083/TCP,8086/TCP 13d  但是我发现一个bug，如果我想要删除非default命名空间下的svc，会返回没有这个svc，就很迷幻。估计是不希望用户随便删除系统的svc。所以如果想删除某个非default命名空间下的svc，请用这个命令。以dashboard为例：\nkubectl delete svc -n kube-system kubernetes-dashboard  用NodePort暴露出来之后，发现并不能像hello-world那样，通过vas-ip:NodePort访问到dashboard。会返回错误。\n This connection is Not Private\nThis website maybe impersionating \u0026lsquo;vps-ip\u0026rsquo; to steal your personcal or financial information. You should go back to the previous page.\n 后面知道这是关于身份验证的问题。那我又搜索了一番，NodePort的方法行不通，试试github k8s官网给的proxy的方法\nkubectl proxy --address=0.0.0.0 --disable-filter=true  使用proxy之后，会在localhost:8001端口监听dashboard请求。这个时候当前的bash就不能使用了：\nubuntu@VM-0-12-ubuntu:~$ sudo kubectl proxy --address=0.0.0.0 --disable-filter=true W0421 16:42:30.534218 23241 proxy.go:167] Request filter disabled, your proxy is vulnerable to XSRF attacks, please be cautious Starting to serve on [::]:8001  然后访问以下网址：\nhttp://148.70.35.123:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#/login\n就可以看到久违一抹dashboard蓝色。到这里我是很兴奋的。然而……这时候login dashboard需要身份验证。UI上也提示了两种验证方式，一种是local file，另一种是Token。我想Token方便一些吧，然后搜索了一下获取Token的方法：\nsudo microk8s.kubectl -n kube-system get secret NAME TYPE DATA AGE kubernetes-dashboard-certs Opaque 0 13d kubernetes-dashboard-csrf Opaque 1 13d kubernetes-dashboard-key-holder Opaque 2 13d kubernetes-dashboard-token-42mpd kubernetes.io/service-account-token 3 13d  Microk8s给出的参考，指向到GitHub k8s的官网对Access Control的介绍，用最下面的kubernetes-dashboard-token-42mpd的Token作为login的密钥。\n查看Token的具体内容用命令\nsudo microk8s.kubectl -n kube-system describe secret kubernetes-dashboard-token-42mpd  就可以看到Token的内容如下：\nubuntu@VM-0-12-ubuntu:~/zecoo$ sudo kubectl -n kube-system describe secret $token Name: default-token-nwqdh Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 20eca024-76ea-4ca7-8fdb-c36594100de8 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IkdlQmtINEEwWmtSVXhsUDA0U000ckJ1alBzZndiNDl5TTNiRml3SWRxeHMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLW53cWRoIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyMGVjYTAyNC03NmVhLTRjYTctOGZkYi1jMzY1OTQxMDBkZTgiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.rhSKv7FnCZMXOJrAJie6xNVnQHDxiCdEC1fVLcdEBfPALhvg7vhnNXuyWJjxE_GaLsK3KoUle5cJCXkpbvcAyOeG14b1pRw_V3LoQsdTwEOrpxZ2VIEI9AC09HL4pEX2b9XSERJMUd1Oua-CJu9rScy1yzKJvdp93MfFDQG0WB6RjuDR_o2imsgfWuERgWX1kkLef1hIIXPtLpA5YrFEhTl40NVS9LBaOlNez49hRq1cd2I6Elc9EY3VuQ5GlVgNAtLht_MAPrrpNuzkxpnLvFYJLCbcqYFpexLF0XarQT5JuCD9F8JNWZanZKPv_hd9uk4pfuZ9XVYU5W2yy7GMvg ca.crt: 1103 bytes  然鹅，我在dashboard UI输入了这串Token，点击login没有任何反应。也不告诉我这串Token是不是出问题了，就是一点反应没有。vps终端倒是有点反应：\n2020/04/21 19:03:42 http: proxy error: context canceled  搜索了一下午，果然用Yandex之后搜索效率高了好多。结果就是：目前为止没有方法可以远程访问k8s的dashboard：[][]讨论1、讨论2、讨论3\n但是好像还有另一种方法：\u0026ndash;enable-skip-login选项可以在dashboard UI中增加一个Skip按钮，跳过login环节。不过这个环节有可能遭到中间人MITM攻击。我担心把力的vps给搞崩了，先不折腾这个了。详情1、详情2\n参考 https://www.jianshu.com/p/6f42ac331d8a （RBAC介绍）\nhttps://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/1.7.x-and-above.md （k8s官方dashboard介绍）\nhttps://microk8s.io/docs/addon-dashboard （microk8s的dashboard介绍）\nhttps://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/README.md#admin-privileges （microk8s指向的k8s access官网）\n","id":29,"section":"posts","summary":"上一节讲到现在可以在vps上访问到hello-world了。 然而我想把访问操作放在我的电脑上。我没有想到NodePort这么容易就能做到了。","tags":["k8s"],"title":"K8S Proxy","uri":"https://zecoo.github.io/hugo/2020/04/k8s-proxy/","year":"2020"},{"content":" 没错这部分也花了我很长时间，而且把我心态还搞崩了。\nk8s安装主要是镜像是国外私有库，pull下来比较麻烦。\n而k8s第一个hello-world难在理解pod、deployment、service之间的关系，以及各种奇奇怪怪的端口映射。我想这是计算机网络基础没有打好埋的坑吧。\n镜像\u0026amp;Pod 首先是创建pod，这里也需要把本地docker创建的image给注射到microk8s.ctr里面。我根据参考1给出的方法构建了hello-node:v1镜像。然后注入到k8s里面。\nimage创建成功之后，就可以构建pod了。用以下命令\nkubectl run hello-node --image=hello-node:v1 --port=8080 --image-pull-policy=Never  创建了一个pod，可以用命令\nkubectl get pods  查看pod的创建。如果创建不成功，可以具体查看pod的详细情况。\nkubectl describe pod hello-node  创建Deployment pod创建成功了，考虑构建deployment，dep是pod的无状态体现。deployment可以控制pod的replica数量。创建方法我没有找到命令行的方式，k8s官网给出的也是推荐使用yaml构建。以下是deployment.yaml文件：\napiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: hello-node spec: selector: matchLabels: name: hello-node replicas: 1 # tells deployment to run 2 pods matching the template template: metadata: labels: name: hello-node spec: containers: - name: hello-node image: hello-node:v1 ports: - containerPort: 8080  有了yaml文件，然后用以下命令可以创建deployment\nkubectl apply -f deployment.yaml  deployment创建成功之后，类似pod，可以用get、describe命令查看指定deployment的情况。\n暴露服务 到目前为止还是不能访问app。因为现在只能在k8s集群内部访问app。我们需要将app变成service才能访问。可以用命令行的方法，也可以用yaml文件的方式。参考中使用的是命令行的方式：\nkubectl expose deployment hello-node --port=8080 --target-port=8080  然后查看service：kubectl get svc hello-node就可以看到service的情况：\nubuntu@VM-0-12-ubuntu:~$ sudo kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-node NodePort 10.152.183.14 \u0026lt;none\u0026gt; 8080:31908/TCP 33m kubernetes ClusterIP 10.152.183.1 \u0026lt;none\u0026gt; 443/TCP 8d  到这里，app就可以在vps中访问了。\nubuntu@VM-0-12-ubuntu:~$ curl 10.152.183.14:8080 Hello World!  但是，仅仅是能在vps中访问。我如果用自己的macbook访问，还需要做其他的代理工作，可能又是一个新的坑等我去补。\n网络端口请补补 最后还有个天坑，等我慢慢补\n本来参考教程可以轻轻松松完成一个k8s的hello-world，但是我自作聪明地把8080端口改成了8081端口。一方面是我怕力会用到这个端口，另一方面是我想8080不是tomcat默认端口嘛，可能会被占用掉。结果就是8081端口让我鼓捣了一整天，pod、deployment、service都是ready状态但就是通过curl 10.152.183.14:8081访问不到。气得我半死。\n也就是说，端口方面的知识还要好好补补。\n还有一个命令，查看端口信息：\nbuntu@VM-0-12-ubuntu:~$ sudo netstat -tlpn Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:44047 0.0.0.0:* LISTEN 17662/containerd tcp 0 0 127.0.0.1:10256 0.0.0.0:* LISTEN 17538/kube-proxy tcp6 0 0 :::80 :::* LISTEN 1276/nginx: master tcp6 0 0 :::7000 :::* LISTEN 1237/frps  虽然这和k8s的demo没有太大关系，还是po出来看看吧。可以看到80端口是力在用的nginx，7000端口是力在用的frps。\n然后我还有想说的：国内Blog质量真的差劲！！！！！我已经不是第一次有这样的感觉了。百度出来的基本上都是垃圾，能解决问题的极少，只会让自己心态崩上加崩。老老实实看官方文档，是效率最高的方式！！！！！\n参考 https://www.cnblogs.com/bforever/p/10601169.html （k8s hello-world）\n","id":30,"section":"posts","summary":"没错这部分也花了我很长时间，而且把我心态还搞崩了。 k8s安装主要是镜像是国外私有库，pull下来比较麻烦。 而k8s第一个hello-worl","tags":["k8s"],"title":"K8S hello-world 回顾","uri":"https://zecoo.github.io/hugo/2020/04/k8s-hello-world/","year":"2020"},{"content":" 写在前面 做到ife task002_3，轮播图花了我最长的时间来写，甚至让我开始反思自己是不是真的…不适合当一个程序员。我要成了程序员，有种侮辱了这个群体的感jio。最后看了一下，虽然也就125行的代码，断断续续我大概做了一周。😓\nhtml\u0026amp;CSS也遇到问题：  怎么把6张图片都塞到frame里面去不漏出来。简单一个overflow：hidden我就是想不到。 6个按钮小圆点怎么放到frame里面。其实还是position的问题，我总是自以为了解position，然而到了真正实践的时候就出问题。小圆点div相对于frame的position要absolute。 如何让图片有一个滑动的效果？我是看了DIYgod的代码之后，才知道要让图片float left，让图片div的宽度仅可能宽，让图片相对于左上角每次移动一部分距离，就能实现移动的效果了。  最要命的是JS部分 一开始我百度了一个轮播图的代码，是通过调整其他图片display：none，指定index的图片display：block。这样就简单实现了图片的切换。这样做也没有什么问题，但就是没有滑动的动画效果，看起来很low。\n干脆继续参考DIYgod的代码。就有了上面提到的，让图片横向展开在页面中。每次切换让图片向左或向右移动一定的距离offset。也就是图片div的left增加或减少offset。如果移动一张图片距离，那么就是offset=img.width。\nNext \u0026amp; Prev button 于是我先把next、prev点击切换的功能做出来，还算简单，主要注意的是next移动到第六张图片的时候，要跳回到第一张图片，要用if做个判断。prev移动到第一张照片的时候同理。\n移动动画效果 到这里可以发现，到目前为止，功能做出来和display：none的那一版没有任何区别。因为移动在一瞬间就完成了，没有形成动画效果。\n如果想要动画效果，要在一定时间time内逐步完成移动。做法是设置每个时间间隔interval移动一小段距离，那么每个时间间隔移动的距离就是offset / (time / interval)。然后设置一个setTimeout函数执行移动过程。\n好，动手操作一下。然后就会惊奇的发现：操，我的轮播图怎么停不下来了？是因为setTimeout之后，每个时间间隔就会执行一次，不设置停止条件的话就会一直执行下去。那么停止条件是什么呢？图片移动了offset的距离，这个动画过程就应该结束。所以停止条件就是initialLeft + offset = newLeft的时候。\n再来看看轮播图效果怎么样？next和prev按钮都可以使用了，而且也有了动画效果。小阶段目标完成，先给自己小小鼓励一下。\n小圆点 接下来考虑实现小圆点和图片的index的同步。这中间也有一些trick，我随便想想发现悟不到就又看了Dg的代码。\n首先考虑怎么做到让某个index的小圆点（下面用O表示）亮起来呢？Dg给出的方法是，给index的O添加一个class比如light，CSS中设置该class light的bgc=white，就相当于让index的O亮起来了，挺巧妙的。同时要注意，亮起来了，还要让他灭掉，也很简单，把所有O的light class都给remove掉。\n好的，现在任意index的O都可以控制其亮或者不亮了。直接把这个方法封装成showBtn()。在next、prev的点击操作中，添加showBtn方法，就可以成功看到指定index的O亮起来了，和图片实现了同步。\n小圆点这里还没完。通常在轮播图中点击某个位置的O，应该能跳转到该index的图片。这里Dg在html中给每个O设置了一个index=0~6属性，通过this.getAttribute()方法获取当前index。有了当前的index，和要移动到的index，相减乘以图片的width，就是要移动的offset。那么到这里这个功能也就实现了。\n所以到最后的顺时针、逆时针播放就更简单了，setInterval()，每隔1000ms执行一次next或者prev，就OK了。但是要注意，如果先后点击playASC和playDESC，会发生鬼畜。解决方法是在这两个方法执行前先判断一下，如果setInterval的timer还在的话，就stop掉。这样鬼畜的问题也就解决了。\n最后 别看我回顾就写了1.3k字，写代码花费的时间真的比我想象中的多多了。感觉那些程序员坐一下午能搞出各种各样的功能，我这……真的菜啊。\n","id":31,"section":"posts","summary":"写在前面 做到ife task002_3，轮播图花了我最长的时间来写，甚至让我开始反思自己是不是真的…不适合当一个程序员。我要成了程序员，有种侮","tags":["js","ife"],"title":"ife 轮播图回顾","uri":"https://zecoo.github.io/hugo/2020/04/ife-%E8%BD%AE%E6%92%AD%E5%9B%BE%E5%9B%9E%E9%A1%BE/","year":"2020"},{"content":" Ubuntu18本身自带snap，可以在microk8s的官网找到安装方法。\n注意k8s的运行条件只要满足以下\n Linux kernel在3.1以上，很多小的openZV的服务器，都是2.6的核儿 Memory要在2G或以上。  我在国外买的xs vps就emmm。再附自己检查kernel的过程\n$ uname -r (check linux kernel) $ dpkg --print-architecture (check linux architecture amd64/arm64)  按照官网的方法，安装成功的话输入sudo kubectl version应该能看到client和server信息。\n但是现在的microk8s还需要安装一些add-on，装这些add-on的时候就会有问题了。\nsudo mircok8s enable dns dashboard\n这里开启microk8s的dns和dashboard服务，但是尝试一下会发现dashboard并不能使用。如果server装了ss可以翻wall，应该不会有这个问题。如果没有ss，用这个命令检查一下namespace为kube-system的pod的情况。\nsudo microk8s.kubectl get pods --all-namespaces\n你会看到以下信息：\nNAMESPACE NAME READY STATUS RESTARTS AGE container-registry registry-7cf58dcdcc-rlfdq 1/1 Running 1 18h default hello-node 0/1 ImagePullBackOff 0 16h kube-system coredns-588fd544bf-nmnnl 1/1 Creating 1 19h kube-system dashboard-metrics-scraper-db65b9c6f-46lrb 1/1 Creating 1 19h kube-system heapster-v1.5.2-58fdbb6f4d-pmgvc 4/4 Creating 4 19h kube-system hostpath-provisioner-75fdc8fccd-5vvfv 1/1 Creating 1 18h kube-system kubernetes-dashboard-67765b55f5-lwrms 1/1 Creating 1 19h kube-system monitoring-influxdb-grafana-v4-6dc675bf8c-lf8nr 2/2 Creating 2 19h  所有namespace为kube-system的pod都是creating的状态。这是一个不可用的状态。再使用这个命令：\nsudo microk8s kubectl describe pods --all-namespaces\n具体查看一下pod的状态，可以看到以下信息；\npull k8s.grc.io/pause:3.1 timeout(具体是什么我忘记提前记录了，总之就是镜像pull不到)  这里又埋下了一个坑。因为k8s.grc.io是Google的私有库，国内访问不到。有两个解决方法：\n 翻wall，上面提到了，如果你可以，那你大概率不会遇到这个问题。 使用国内镜像pull下来这几个镜像然后重新打tag  我选择硬刚第二种方法。\n国内镜像源可用的我看了一下，大概有阿里云、中科院和Azure.cn这三个。\n先记录一个pull image然后打tag的过程吧。\ndocker pull gcr.mirrors.ustc.edu.cn/google-containers/pause:3.1 docker tag gcr.mirrors.ustc.edu.cn/google-containers/pause:3.1 k8s.grc.io/pause:3.1  讲道理，到这里应该已经成了，但是microk8s不认刚装上的docker怎么办？我想这里应该也有两个解决方法\n 让k8s和docker公用一个registry，打通他们之间的通道 microk8s自己也有docker，把pull下来的image注射进去  第一个方法我懒得找了，用的第二种方法。在这里还要说一下microk8s的一个坑：最新的版本（我用的是1.18.0）是不能直接用microk8s.docker操作docker的，好像版本1.13之前的可以。这样就可以直接用microk8s的docker拉取镜像然后重新打tag就好了。新版本想要使用docker，具体的注射方法是这样的：\ndocker save k8s.grc.io/pause:3.1 \u0026gt; pause.tar  然后ls一下可以看到目录下面多了一个pause.tar，先压缩然后再注入到microk8s中\nsudo microk8s.ctr image import pause.tar  这个命令让俺搜了很长时间。更要命是不管是baidu还是bing上关于microk8s.ctr的内容都几乎没有。然后我看了下ctr image的help，里面有一个check命令可以查看ctr中的image，相当于docker images命令。\nsudo microk8s.ctr image check  就可以看到刚刚的k8s.grc.io/pause:3.1已经import进去了。\n然鹅，因为墙的问题需要import的镜像还有好多呢。一个一个import进去，疯了。然后就看到了批量import镜像的bash脚本。贴在下面：\n#!/bin/bash images=( k8s.gcr.io/pause:3.1=gcr.azk8s.cn/google-containers/pause:3.1 gcr.io/google_containers/defaultbackend-amd64:1.4=gcr.azk8s.cn/google-containers/defaultbackend-amd6 4:1.4 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1=registry.cn-hangzhou.aliyuncs.com/google_containers/ku bernetes-dashboard-amd64:v1.10.1 k8s.gcr.io/heapster-influxdb-amd64:v1.3.3=registry.cn-hangzhou.aliyuncs.com/google_containers/heapst er-influxdb-amd64:v1.3.3 k8s.gcr.io/heapster-amd64:v1.5.2=registry.cn-hangzhou.aliyuncs.com/google_containers/heapster-amd64: v1.5.2 k8s.gcr.io/heapster-grafana-amd64:v4.4.3=registry.cn-hangzhou.aliyuncs.com/google_containers/heapste r-grafana-amd64:v4.4.3 ) OIFS=$IFS; # ?~]?~X?~W??~@? for image in ${images[@]};do IFS='=' set $image docker pull $2 docker tag $2 $1 docker rmi $2 docker save $1 \u0026gt; 1.tar \u0026amp;\u0026amp; microk8s.ctr image import 1.tar \u0026amp;\u0026amp; rm 1.ta r IFS=$OIFS; # ?~X?~N~_?~W??~@? done  先用microk8s.ctr image check检查一下是不是都注入进去了。\n然后重新启动microk8s，再次检查namespace为kube-system的pod的状态，应该可以看到状态从一直Creating变成了ready。到这里应该可以说安装算是结束了。\n最后总结一下吧，核心其实就是k8s需要Google私有库的image支持，但是因为网络原因获取不到，需要用其他方法把这些image装到k8s里。以上所有的内容都是在踩坑。md浪费了老子好多时间。\n参考： https://zhuanlan.zhihu.com/p/81648464\nhttps://www.cnblogs.com/xiao987334176/p/10931290.html （镜像源）\nhttps://segmentfault.com/a/1190000019534913?utm_source=tag-newest\nhttps://www.bayun.org/article/11 （脚本批量import镜像）\nhttps://blog.csdn.net/championzgj/article/details/93299777 （曲线救国）\nhttps://www.dazhuanlan.com/2019/12/12/5df13665d9eee/ （microk8s基本命令）\nhttps://blog.csdn.net/shykevin/article/details/90703428\n","id":32,"section":"posts","summary":"Ubuntu18本身自带snap，可以在microk8s的官网找到安装方法。 注意k8s的运行条件只要满足以下 Linux kernel在3.1以上，很多","tags":["k8s","mirok8s"],"title":"Microk8s 国内安装总结","uri":"https://zecoo.github.io/hugo/2020/04/microk8s-installation-in-china/","year":"2020"},{"content":" 弄起来挺快的，其实就三点\n 安装ss。apt install就可以 配置ss。就一个json文件 启动ss。一行命令  参考： https://viencoding.com/article/90\n","id":33,"section":"posts","summary":"弄起来挺快的，其实就三点 安装ss。apt install就可以 配置ss。就一个json文件 启动ss。一行命令 参考： https://viencoding.com/article/90","tags":["server","ss"],"title":"SS on Ubuntu","uri":"https://zecoo.github.io/hugo/2020/04/ss-on-ubuntu/","year":"2020"},{"content":"在ife上也写了几个网页了，但是如果一直用github-pages的话，感觉有点麻烦，自己有一个服务器干嘛不直接用呢？\n安装Nginx及其简单：\napt install nginx\n这个时候如果发现终端不动了，没啥反应，其实是Nginx已经启动了，浏览器输入地址看看是不是能看到nginx欢迎页。\n下一步给nginx设置自己的主页\n按照参考博客给出的思路，先建立一个文件夹作为nginx访问目录。比如/www/static-web\n然后给自己的目的设置一个nginx配置文件，配置文件地址\ncd /etc/nginx/conf.d/\n然后新建一个static-hello.conf\n写进以下内容：\nserver { server_name = 89.33.194.100; // 你自己的地址或者域名 root /root/www/static-web; // nginx访问目录 index index.html; location ~* ^.+\\.(jpg|jpeg|gif|png|ico|css|js|pdf|txt){ root /root/www/static-web/; } }  然后重启nginx\nnginx -s reload\n我在浏览器访问了一下自己的地址，发现403 Forbidden。故排查之。\n查看nginx错误信息\ncat /var/log/nginx/error.log\n得到这样的信息\n2020/04/08 23:43:58 [notice] 1390#1390: signal process started 2020/04/08 23:44:10 [error] 1391#1391: *4 \u0026quot;/root/www/static-web/index.html\u0026quot; is forbidden (13: Permission denied), client: 171.41.91.51, server: =, request: \u0026quot;GET / HTTP/1.1\u0026quot;, host: \u0026quot;89.33.194.100\u0026quot;  说明权限不够，故搜索解决方案\n找到编辑nginx配置文件（一般是/etc/nginx/nginx.conf）。然后我在开头就找到问题了\nroot@zecoo:/etc/nginx# cat nginx.conf user www-data; worker_processes auto; pid /run/nginx.pid;  user 是www-data不是root，更改之后重启，就能看到指定目录下的index.html了。\n参考： https://segmentfault.com/a/1190000010487262?utm_source=tag-newest\nhttps://www.liangjucai.com/article/224\n","id":34,"section":"posts","summary":"在ife上也写了几个网页了，但是如果一直用github-pages的话，感觉有点麻烦，自己有一个服务器干嘛不直接用呢？ 安装Nginx及其简单","tags":["nginx"],"title":"初试Nginx部署静态网页","uri":"https://zecoo.github.io/hugo/2020/04/%E5%88%9D%E8%AF%95nginx%E9%83%A8%E7%BD%B2%E9%9D%99%E6%80%81%E7%BD%91%E9%A1%B5/","year":"2020"},{"content":" task1的时候我还能勉强用自己的想法写，html的展示效果达到了就好。\n但是到了task2的时候，js基础还是不牢啊，基本上每道题都要看下别人的博客。虽然有点low，但是还是秉着一股劲往前冲，后面再来捡技术细节的思路，不丢人的。\n一趟捣鼓下来，JS部分一共就5个板块：\n js基础：数据结构、对象等 dom（html树形结构操作） ajax（和服务器交互，部分刷新页面） jQuery（更方便操作dom和ajax） nodejs（暂时不学习）  接下来总结一下自己的捣鼓过程吧（不包含技术细节）\n任务2 JS基础 判断数据类型。 用Objexct.prototype来判断\n深克隆。 如果直接var copy = src就是浅复制，所以要用递归的形式。\n数组相关操作 1 simpleTrim。这种问题以前从来没接触过，不会写正常2333。for循环判断分别从前和从后面判断时候有空字符\n2 trim。正则表达式直接把空字符给替换掉\n正则表达式 这是我第几次学习正则表达式啦？？\n任务3 Dom 到这里最好还是结合html页面来学习，直接上来写老夫实在是无处下手。\n任务4 Event 同dom\n任务5 Bom cookie的设置和获取。\n以前只知道cookie是什么，现在知道cookie长什么样了怎么搞一个。\n任务6 Ajax emmm自己写一个简单的，讲道理，其实是xmlhttp的使用，和ajax没有什么关系。\n","id":35,"section":"posts","summary":"task1的时候我还能勉强用自己的想法写，html的展示效果达到了就好。 但是到了task2的时候，js基础还是不牢啊，基本上每道题都要看下别","tags":["js","ife"],"title":"IFE js.util","uri":"https://zecoo.github.io/hugo/2020/03/ife-js-util/","year":"2020"},{"content":" 我记录一下ife其他几个页面。一共这么几个页面\n index.html post.html archive.html about.html  如果自己写的话，除了index.html，其他几个页面都有或多多少的问题我写不出来。 一个一个说吧。\nIndex.html  banner有三个部分，一个是logo，一个是站内其他链接，最后一个是git的logo。这三个部分要显示在水平的一行，这里其实是运用到了float，只要把三个部分都设置为float，就可以变成一行了。感觉有点像把div改成了块内元素一样，也就是变成了span。 一般banner的链接都是用list改成水平，改成水平的方法有两个，一个是float，另一个是display inline-block 我想让这三部分，随着页面的改变，位置不要定死，原来margin-left之类的也可以用百分比来表示，就很舒服了。 center是一个大的bg，bg上面有一个透明度的方形显示文字，放bg的时候遇到了以前就遇到的问题，比如默认是重复的，而且不能充满div。这两个问题代码很好解决。 至于透明度的方形，彻底让我对position这个元素有了一定的理解。absolute、fixed、relative分别代表什么。 box部分是三个卡片，卡片里有图片和文字。三个卡片并列其实也挺好写，好像用inline-block就可以实现，水平垂直居中什么的也都是现成的，不过写好了之后再看要求里面有一个是卡片的高度随文字内容自适应。 遂百度之，发现用display table可以解决这个问题，但是table是占满整个div的，不过也好解决，用一大一小div就解决了。 圆形的头像，这个做起来也好简单啊，图片的radius改一下就可以了。 intro部分其实不用display来写，比较常规。如果需要居中之类的工作，用display明显是要方便很多的。 最后一点，如果用flex来布局，flex里面的每个元素的宽度都是以最宽的那个为标准。这时候想要居中的话可以考虑用text-align center解决。这是最后那个小logo的居中问题。  Post.html 在开始写post.html之前我在GitHub上down了几个其他人的ife作业，然后发现我这个代码真的是稀烂…最基本的有这么几个问题：\n 文件的结构，ife里强调了，但是我没有遵守。 div的命名方式。navbar，navbar-menu、profile之类的 class和id的区别 nav和footer是html的元素，可以不用div写 html的基本框架应该是navbar、banner、content、footer  然后我找到了一个极佳的对比例子。就是DIYgod，当我看到他就是RSSHub的发起者的时候，我懵逼了。他也就大我三届而已，而且还是武理的，我简直太菜了。。酸归酸，看DIYgod大神的前端页面。能学到很多东西：\n html的其他默认元素：header、section、article、nav的使用 日历写起来其实就是一个table view tag-graph就更简单了，字体设几个不同的大小就可以 一个简单的单词，换一个字体就能变成logo一般的存在  然后记录一下写 blog.html的时候遇到的坑\n blog应该是一个简单的双列布局。双列布局的形式有哪些来着？我是用百分比的形式来布局的。要求用980px，我不太喜欢 table设置了宽高之后，cell变大，那么cell里面字的间距自然就大了 有时候会遇到给div里面子元素加padding之后，div的宽度哪怕设好了，还是会超出来。这个时候要设置一下box-sizing=border-content a写在div外面，就把整个div变成了链接。 我现在使用自适应的方法太简陋了，@media (maxwidth=980) {.div { display=none} }  Archive.html 这个页面我被卡住了。本来一个瀑布流我可以百度，用flex或者用column来解决，但是ife的作业的瀑布流里偏偏有一个比较大的块，是一个这样的结构\n[ + ][ ][ ]\n[ ][ ][ ][ ]\n[ ][ ][ ][ ]\n就比较棘手。不能直接用简单的瀑布流，最后是划成3个部分，1⃣️是 [ + ] 2⃣️是其下面的部分，3⃣️是右边两排。然后让2⃣️和3⃣️变成瀑布流。然后细细碎碎的一些对齐问题啊、宽度计算的问题啊，浪费了我挺多时间。\n然后更吊诡的是，我连一个简单的3个div并排在一排都写不出来。好像用inline-block就能写出来。不过我还要再整理一下思路。\nAbout.html 不得不说ife作业的设置真的很好，about.html暴露出来我的另一个问题，其实什么position的我根本没有搞清楚，稍微复杂一点的定位我就搞不定了。\nrelative和absolute的依据是什么？ 如果加上了flex布局又会受什么影响？  ","id":36,"section":"posts","summary":"我记录一下ife其他几个页面。一共这么几个页面 index.html post.html archive.html about.html 如果自己写的话，除了index.html，其他几个页面都有或多多少的问题我写不出来。 一","tags":["css","ife"],"title":"IFE Task0001 Log","uri":"https://zecoo.github.io/hugo/2020/03/ife-task0001-log/","year":"2020"},{"content":" Microservices Monitoring with Event Logs and Black Box Execution Tracing\nTSC 2019\nChaper1 理解log一般需要专家，文章提出一个novel的方式帮助troubleshooting的决策 Clearwater IP Multimedia Subsystem 和 hipster差不多，都是有十多个微服务的玩意儿 嗯，微服务decomposed，非常适合monitoring，同时也有挑战 indirect monitoring 检测cpu之类的什么东西，direct monitoring可以直接在代码里插入log function进行monitoring。event log属于direct monitoring 现有的log management tool应该用的是正则和关键词查找的方式做分析，但是好像不是特别适合微服务。毕竟不同的系统，分析规则是不一样的 吹一波作者的monitoring工具MetroFunnel，不需要改变现有的微服务，也不需要对现有的系统有特别深刻的了解。\nChapter2 介绍indirect monitoring和direct monitoring，了解一下即可。Zipkin就是direct monitoring的方法\nChapter3 给了很多例子 第一个例子的错误是clearwater客户尝试拨打语音电话发生错误 ClearWater IP Multimedia Subsystem IMS IP多媒体子系统 安装 bono是IMS的一个微服务，负责客户端。同时发现sprout和bono接近，也是间接和客户端有关。又转向另一个微服务homestead，在这个里面找到了error，但是也没有说为什么就找到了homestead。这一章就讲了在不同的微服务里debug，结果还没说清楚微服务是怎么进行选择的。fk 有用的log比较少，在ms里情况更甚。\nChapter4 Goals tracing tech应该第一，不会产生新的服务；第二，不会对代码有影响；第三，没有很重的配置文件。 还有一些non-goals作者讲得很清楚。\nChapter5 感觉内容挺多？ 因为微服务一般都是用REST形式的api，就可以通过抓包的形式，也就是passive tracing的方式来进行monitoring，但这在之前的一些论文中已经有体现了。\nFormat of trace trace是代表微服务调用结果的记录序列。一般包含很多信息，比如时间戳、地址、ip、response code等，文章都有列出来\nTracing algorithm Tracing Algorithm有两个要点，\n第一个是capture packets。简单判断一下就可以，正常的发送是GET /test/users/1这样的。接收到的是这样的HTTP/1.1 200 OK\n第二个是timeout cheker pending list：requests to be decisioned expiration of a request：插入后等待的时间过长，就expire termination of a request：request来了等待的时间过长，就terminate\nChapter6 Clearwater的每个组件也介绍了。bono代表连接成功，cassandra是数据库。 Clearwater这个实验环境的搭建我还不能彻底理解，可以多看两遍 然后介绍了k8s，最后实验环境里还是要用到k8s作balancer\nChpater7 讲的是一些具体的case，比如一开始提到的502错误，MetroFunnel能直接给出结果。另外还有overload问题，\nChapter8 第八章，实验证明第一，MF不会对Clearwater的性能有影响，Kubernetes的影响也很小。第二，能显著减少log file的大小。\n","id":37,"section":"posts","summary":"Microservices Monitoring with Event Logs and Black Box Execution Tracing TSC 2019 Chaper1 理解log一般需要专家，文章提出一个novel的方式帮助troubleshooting的决策 Clearwater IP Multimedia Subsystem 和 hipste","tags":[""],"title":"Monitoring with log and trace","uri":"https://zecoo.github.io/hugo/1/01/monitoring-with-log-and-trace/","year":"0001"}],"tags":[{"title":"Tags","uri":"https://zecoo.github.io/hugo/tags/"},{"title":"bash","uri":"https://zecoo.github.io/hugo/tags/bash/"},{"title":"blog","uri":"https://zecoo.github.io/hugo/tags/blog/"},{"title":"css","uri":"https://zecoo.github.io/hugo/tags/css/"},{"title":"decomposition","uri":"https://zecoo.github.io/hugo/tags/decomposition/"},{"title":"hello-world","uri":"https://zecoo.github.io/hugo/tags/hello-world/"},{"title":"highlight","uri":"https://zecoo.github.io/hugo/tags/highlight/"},{"title":"HPA","uri":"https://zecoo.github.io/hugo/tags/hpa/"},{"title":"hugo","uri":"https://zecoo.github.io/hugo/tags/hugo/"},{"title":"icws","uri":"https://zecoo.github.io/hugo/tags/icws/"},{"title":"ife","uri":"https://zecoo.github.io/hugo/tags/ife/"},{"title":"istio","uri":"https://zecoo.github.io/hugo/tags/istio/"},{"title":"jq","uri":"https://zecoo.github.io/hugo/tags/jq/"},{"title":"js","uri":"https://zecoo.github.io/hugo/tags/js/"},{"title":"k8s","uri":"https://zecoo.github.io/hugo/tags/k8s/"},{"title":"kubeadm","uri":"https://zecoo.github.io/hugo/tags/kubeadm/"},{"title":"kubectl","uri":"https://zecoo.github.io/hugo/tags/kubectl/"},{"title":"math","uri":"https://zecoo.github.io/hugo/tags/math/"},{"title":"mirok8s","uri":"https://zecoo.github.io/hugo/tags/mirok8s/"},{"title":"nginx","uri":"https://zecoo.github.io/hugo/tags/nginx/"},{"title":"prom","uri":"https://zecoo.github.io/hugo/tags/prom/"},{"title":"RCA","uri":"https://zecoo.github.io/hugo/tags/rca/"},{"title":"scss","uri":"https://zecoo.github.io/hugo/tags/scss/"},{"title":"server","uri":"https://zecoo.github.io/hugo/tags/server/"},{"title":"ss","uri":"https://zecoo.github.io/hugo/tags/ss/"},{"title":"valueless","uri":"https://zecoo.github.io/hugo/tags/valueless/"}]}